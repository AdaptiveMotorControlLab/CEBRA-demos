{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5dfc38dc",
   "metadata": {},
   "source": [
    "\n",
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70d7c62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib as jl\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras import backend as K\n",
    "\n",
    "sys.path.insert(0, '..')\n",
    "import cebra.datasets\n",
    "\n",
    "sys.path.insert(0, '../third_party/pivae')\n",
    "import pivae_code.datasets, pivae_code.conv_pi_vae, pivae_code.pi_vae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee6278b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Set your own seed\n",
    "seed = 42\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36bb47f",
   "metadata": {},
   "source": [
    "## Load and generate dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aeb14cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_NAME = 'rat-hippocampus-achilles-3fold-trial-split-0'\n",
    "offset_right = 5\n",
    "offset_left = 5\n",
    "\n",
    "def _call_dataset(offset_right, offset_left, split):\n",
    "    dataset = cebra.datasets.init(DATA_NAME, split = split)\n",
    "    dataset.offset.right = offset_right\n",
    "    dataset.offset.left = offset_left\n",
    "    return dataset\n",
    "\n",
    "train_set = _call_dataset(offset_right, offset_left, 'train')\n",
    "valid_set=_call_dataset(offset_right, offset_left, 'valid')\n",
    "test_set=_call_dataset(offset_right, offset_left, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "532369c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_loader(dataset, batch_size):\n",
    "    def _to_batch_list(x, y, batch_size):\n",
    "        if x is not None and y is not None:\n",
    "            x = x.squeeze()\n",
    "            if len(x.shape) == 3:\n",
    "                x = x.transpose(0,2,1) \n",
    "            x_batch_list = np.array_split(x, int(len(x) / batch_size))\n",
    "            y_batch_list = np.array_split(y, int(len(y) / batch_size))\n",
    "        else:\n",
    "            return None, None\n",
    "        return x_batch_list, y_batch_list\n",
    "    \n",
    "    x,u = _to_batch_list(dataset[torch.arange(len(dataset))].numpy(), dataset.index.numpy(), batch_size)\n",
    "    \n",
    "    loader = pivae_code.pi_vae.custom_data_generator(x, u)\n",
    "    _len = len(x)\n",
    "    return x, u, loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a5f5c477",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 200\n",
    "train_x, train_u, train_loader = make_loader(train_set, batch_size)\n",
    "valid_x, valid_u, valid_loader = make_loader(valid_set, batch_size)\n",
    "test_x, test_u, test_loader = make_loader(test_set, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe85ca0",
   "metadata": {},
   "source": [
    "## Visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e267dbd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 900x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(9,3))\n",
    "plt.subplots_adjust(wspace = 0.3)\n",
    "ax = plt.subplot(121)\n",
    "ax.imshow(train_set.neural.numpy()[:1000].T, aspect = 'auto', cmap = 'gray_r')\n",
    "plt.ylabel('Neuron #')\n",
    "plt.xlabel('Time [s]')\n",
    "plt.xticks(np.linspace(0, 1000, 5), np.linspace(0, 0.025*1000, 5, dtype = int))\n",
    "\n",
    "ax2 = plt.subplot(122)\n",
    "ax2.scatter(np.arange(1000), train_set.continuous_index[:1000,0], c = 'gray', s=1)\n",
    "plt.ylabel('Position [m]')\n",
    "plt.xlabel('Time [s]')\n",
    "plt.xticks(np.linspace(0, 1000, 5), np.linspace(0, 0.025*1000, 5, dtype = int))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4819ceac",
   "metadata": {},
   "source": [
    "## Define and train a conv-piVAE model\n",
    "- Note that conv-piVAE use 10 time bins (250ms) receptive field, while piVAE uses 1 time bin (25ms). \n",
    "- If training takes too long, use smaller training epochs (usually the loss converges around ~300 epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4b3c71c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-03 14:33:40.035925: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-10-03 14:33:40.035974: E tensorflow/stream_executor/cuda/cuda_driver.cc:313] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-10-03 14:33:40.036015: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (stes-t14s): /proc/driver/nvidia/version does not exist\n",
      "2022-10-03 14:33:40.036392: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2022-10-03 14:33:40.051370: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 1697430000 Hz\n",
      "2022-10-03 14:33:40.053336: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fd814000b70 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2022-10-03 14:33:40.053404: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vae\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 10, 120)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 3)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder (Model)                 [(None, 2), (None, 2 35020       input_1[0][0]                    \n",
      "                                                                 input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "decoder (Model)                 (None, 10, 120)      2977794     encoder[1][2]                    \n",
      "==================================================================================================\n",
      "Total params: 3,012,814\n",
      "Trainable params: 3,012,814\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stes/.conda/envs/cebra-full/lib/python3.8/site-packages/keras/engine/training_utils.py:816: UserWarning: Output encoder missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to encoder.\n",
      "  warnings.warn(\n",
      "/home/stes/.conda/envs/cebra-full/lib/python3.8/site-packages/keras/engine/training_utils.py:816: UserWarning: Output decoder missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to decoder.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "conv_pivae = pivae_code.conv_pi_vae.conv_vae_mdl(\n",
    "                dim_x=train_set.neural.shape[1],\n",
    "                dim_z=2,\n",
    "                dim_u=3,\n",
    "                time_window=10,\n",
    "                gen_nodes=60,\n",
    "                n_blk=2,\n",
    "                mdl=\"poisson\",\n",
    "                disc=False,\n",
    "                learning_rate=0.00025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "77bcb091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/600\n",
      "34/34 [==============================] - 3s 100ms/step - loss: 509.8190 - val_loss: 212.0420\n",
      "Epoch 2/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 212.5554 - val_loss: 177.2430\n",
      "Epoch 3/600\n",
      "34/34 [==============================] - 2s 45ms/step - loss: 192.3327 - val_loss: 165.8879\n",
      "Epoch 4/600\n",
      "34/34 [==============================] - 1s 42ms/step - loss: 186.7391 - val_loss: 164.7504\n",
      "Epoch 5/600\n",
      "34/34 [==============================] - 1s 43ms/step - loss: 186.4515 - val_loss: 164.5008\n",
      "Epoch 6/600\n",
      "34/34 [==============================] - 1s 43ms/step - loss: 186.4154 - val_loss: 164.6330\n",
      "Epoch 7/600\n",
      "34/34 [==============================] - 1s 43ms/step - loss: 186.4084 - val_loss: 164.6232\n",
      "Epoch 8/600\n",
      "34/34 [==============================] - 1s 44ms/step - loss: 186.3944 - val_loss: 164.5316\n",
      "Epoch 9/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 186.2701 - val_loss: 164.2339\n",
      "Epoch 10/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 186.1861 - val_loss: 164.2754\n",
      "Epoch 11/600\n",
      "34/34 [==============================] - 1s 44ms/step - loss: 186.1450 - val_loss: 164.1155\n",
      "Epoch 12/600\n",
      "34/34 [==============================] - 1s 43ms/step - loss: 186.0446 - val_loss: 164.0661\n",
      "Epoch 13/600\n",
      "34/34 [==============================] - 1s 43ms/step - loss: 185.9379 - val_loss: 164.1499\n",
      "Epoch 14/600\n",
      "34/34 [==============================] - 1s 43ms/step - loss: 185.8697 - val_loss: 164.0963\n",
      "Epoch 15/600\n",
      "34/34 [==============================] - 2s 45ms/step - loss: 185.6543 - val_loss: 163.8571\n",
      "Epoch 16/600\n",
      "34/34 [==============================] - 2s 45ms/step - loss: 185.2769 - val_loss: 163.7182\n",
      "Epoch 17/600\n",
      "34/34 [==============================] - 1s 43ms/step - loss: 184.5498 - val_loss: 163.2224\n",
      "Epoch 18/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 182.9443 - val_loss: 163.1278\n",
      "Epoch 19/600\n",
      "34/34 [==============================] - 2s 47ms/step - loss: 180.7850 - val_loss: 161.5421\n",
      "Epoch 20/600\n",
      "34/34 [==============================] - 2s 44ms/step - loss: 178.7081 - val_loss: 159.5752\n",
      "Epoch 21/600\n",
      "34/34 [==============================] - 1s 44ms/step - loss: 176.8280 - val_loss: 158.1187\n",
      "Epoch 22/600\n",
      "34/34 [==============================] - 1s 43ms/step - loss: 174.9552 - val_loss: 155.8834\n",
      "Epoch 23/600\n",
      "34/34 [==============================] - 1s 44ms/step - loss: 173.0196 - val_loss: 152.2670\n",
      "Epoch 24/600\n",
      "34/34 [==============================] - 2s 48ms/step - loss: 170.3790 - val_loss: 149.4572\n",
      "Epoch 25/600\n",
      "34/34 [==============================] - 2s 45ms/step - loss: 168.4063 - val_loss: 147.2516\n",
      "Epoch 26/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 166.8510 - val_loss: 144.7695\n",
      "Epoch 27/600\n",
      "34/34 [==============================] - 1s 44ms/step - loss: 165.3624 - val_loss: 143.2796\n",
      "Epoch 28/600\n",
      "34/34 [==============================] - 1s 43ms/step - loss: 164.1374 - val_loss: 142.2919\n",
      "Epoch 29/600\n",
      "34/34 [==============================] - 1s 44ms/step - loss: 163.0654 - val_loss: 141.1287\n",
      "Epoch 30/600\n",
      "34/34 [==============================] - 1s 43ms/step - loss: 162.3048 - val_loss: 140.3004\n",
      "Epoch 31/600\n",
      "34/34 [==============================] - 1s 44ms/step - loss: 161.2343 - val_loss: 139.6303\n",
      "Epoch 32/600\n",
      "34/34 [==============================] - 1s 43ms/step - loss: 160.4963 - val_loss: 138.8364\n",
      "Epoch 33/600\n",
      "34/34 [==============================] - 1s 43ms/step - loss: 159.7594 - val_loss: 137.9424\n",
      "Epoch 34/600\n",
      "34/34 [==============================] - 1s 44ms/step - loss: 159.0830 - val_loss: 137.6932\n",
      "Epoch 35/600\n",
      "34/34 [==============================] - 1s 43ms/step - loss: 158.5559 - val_loss: 137.3128\n",
      "Epoch 36/600\n",
      "34/34 [==============================] - 1s 43ms/step - loss: 157.8401 - val_loss: 136.6159\n",
      "Epoch 37/600\n",
      "34/34 [==============================] - 1s 44ms/step - loss: 157.4032 - val_loss: 136.8394\n",
      "Epoch 38/600\n",
      "34/34 [==============================] - 1s 43ms/step - loss: 156.8842 - val_loss: 136.2114\n",
      "Epoch 39/600\n",
      "34/34 [==============================] - 1s 43ms/step - loss: 156.2586 - val_loss: 135.7509\n",
      "Epoch 40/600\n",
      "34/34 [==============================] - 1s 43ms/step - loss: 155.9265 - val_loss: 135.3052\n",
      "Epoch 41/600\n",
      "34/34 [==============================] - 1s 43ms/step - loss: 155.5301 - val_loss: 135.3062\n",
      "Epoch 42/600\n",
      "34/34 [==============================] - 1s 43ms/step - loss: 155.1858 - val_loss: 134.9559\n",
      "Epoch 43/600\n",
      "34/34 [==============================] - 1s 43ms/step - loss: 154.7895 - val_loss: 135.1275\n",
      "Epoch 44/600\n",
      "34/34 [==============================] - 1s 43ms/step - loss: 154.4883 - val_loss: 135.5404\n",
      "Epoch 45/600\n",
      "34/34 [==============================] - 1s 43ms/step - loss: 154.2749 - val_loss: 135.3865\n",
      "Epoch 46/600\n",
      "34/34 [==============================] - 1s 43ms/step - loss: 153.9525 - val_loss: 135.5557\n",
      "Epoch 47/600\n",
      "34/34 [==============================] - 2s 44ms/step - loss: 153.7735 - val_loss: 134.6591\n",
      "Epoch 48/600\n",
      "34/34 [==============================] - 1s 43ms/step - loss: 153.4866 - val_loss: 134.1797\n",
      "Epoch 49/600\n",
      "34/34 [==============================] - 1s 43ms/step - loss: 153.2572 - val_loss: 134.4760\n",
      "Epoch 50/600\n",
      "34/34 [==============================] - 1s 43ms/step - loss: 153.0343 - val_loss: 134.2853\n",
      "Epoch 51/600\n",
      "34/34 [==============================] - 1s 42ms/step - loss: 152.7185 - val_loss: 134.2296\n",
      "Epoch 52/600\n",
      "34/34 [==============================] - 1s 43ms/step - loss: 152.7369 - val_loss: 134.3609\n",
      "Epoch 53/600\n",
      "34/34 [==============================] - 1s 43ms/step - loss: 152.4475 - val_loss: 133.9866\n",
      "Epoch 54/600\n",
      "34/34 [==============================] - 2s 47ms/step - loss: 152.3859 - val_loss: 134.5579\n",
      "Epoch 55/600\n",
      "34/34 [==============================] - 2s 51ms/step - loss: 152.2696 - val_loss: 134.0202\n",
      "Epoch 56/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 152.1087 - val_loss: 133.5061\n",
      "Epoch 57/600\n",
      "34/34 [==============================] - 2s 53ms/step - loss: 151.8830 - val_loss: 134.3623\n",
      "Epoch 58/600\n",
      "34/34 [==============================] - 2s 48ms/step - loss: 151.6815 - val_loss: 133.9002\n",
      "Epoch 59/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 151.6416 - val_loss: 134.3605\n",
      "Epoch 60/600\n",
      "34/34 [==============================] - 2s 53ms/step - loss: 151.5324 - val_loss: 133.6862\n",
      "Epoch 61/600\n",
      "34/34 [==============================] - 2s 51ms/step - loss: 151.2587 - val_loss: 133.8059\n",
      "Epoch 62/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 151.2168 - val_loss: 134.0073\n",
      "Epoch 63/600\n",
      "34/34 [==============================] - 2s 45ms/step - loss: 151.2193 - val_loss: 134.2295\n",
      "Epoch 64/600\n",
      "34/34 [==============================] - 2s 45ms/step - loss: 150.9878 - val_loss: 134.1403\n",
      "Epoch 65/600\n",
      "34/34 [==============================] - 2s 45ms/step - loss: 150.8001 - val_loss: 134.5686\n",
      "Epoch 66/600\n",
      "34/34 [==============================] - 2s 45ms/step - loss: 150.9648 - val_loss: 134.7472\n",
      "Epoch 67/600\n",
      "34/34 [==============================] - 2s 45ms/step - loss: 150.8419 - val_loss: 135.1183\n",
      "Epoch 68/600\n",
      "34/34 [==============================] - 2s 44ms/step - loss: 151.0243 - val_loss: 135.6888\n",
      "Epoch 69/600\n",
      "34/34 [==============================] - 2s 45ms/step - loss: 150.8993 - val_loss: 135.2122\n",
      "Epoch 70/600\n",
      "34/34 [==============================] - 2s 45ms/step - loss: 151.0090 - val_loss: 135.0504\n",
      "Epoch 71/600\n",
      "34/34 [==============================] - 1s 43ms/step - loss: 150.9658 - val_loss: 134.8042\n",
      "Epoch 72/600\n",
      "34/34 [==============================] - 1s 43ms/step - loss: 150.7124 - val_loss: 134.5691\n",
      "Epoch 73/600\n",
      "34/34 [==============================] - 1s 43ms/step - loss: 150.4768 - val_loss: 134.2057\n",
      "Epoch 74/600\n",
      "34/34 [==============================] - 1s 42ms/step - loss: 150.3804 - val_loss: 134.2023\n",
      "Epoch 75/600\n",
      "34/34 [==============================] - 1s 43ms/step - loss: 150.2365 - val_loss: 133.7696\n",
      "Epoch 76/600\n",
      "34/34 [==============================] - 1s 43ms/step - loss: 150.1960 - val_loss: 133.8051\n",
      "Epoch 77/600\n",
      "34/34 [==============================] - 1s 44ms/step - loss: 149.9340 - val_loss: 133.0406\n",
      "Epoch 78/600\n",
      "34/34 [==============================] - 2s 45ms/step - loss: 149.9740 - val_loss: 132.7862\n",
      "Epoch 79/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34/34 [==============================] - 2s 48ms/step - loss: 149.7775 - val_loss: 133.0508\n",
      "Epoch 80/600\n",
      "34/34 [==============================] - 2s 44ms/step - loss: 149.6358 - val_loss: 132.8242\n",
      "Epoch 81/600\n",
      "34/34 [==============================] - 1s 43ms/step - loss: 149.8333 - val_loss: 132.9957\n",
      "Epoch 82/600\n",
      "34/34 [==============================] - 1s 43ms/step - loss: 149.6818 - val_loss: 134.0401\n",
      "Epoch 83/600\n",
      "34/34 [==============================] - 1s 43ms/step - loss: 150.2907 - val_loss: 133.8262\n",
      "Epoch 84/600\n",
      "34/34 [==============================] - 1s 43ms/step - loss: 150.5345 - val_loss: 134.5123\n",
      "Epoch 85/600\n",
      "34/34 [==============================] - 1s 44ms/step - loss: 150.5716 - val_loss: 134.5144\n",
      "Epoch 86/600\n",
      "34/34 [==============================] - 1s 43ms/step - loss: 150.1075 - val_loss: 134.8088\n",
      "Epoch 87/600\n",
      "34/34 [==============================] - 1s 44ms/step - loss: 149.7634 - val_loss: 134.5372\n",
      "Epoch 88/600\n",
      "34/34 [==============================] - 1s 44ms/step - loss: 149.4065 - val_loss: 135.4284\n",
      "Epoch 89/600\n",
      "34/34 [==============================] - 1s 44ms/step - loss: 149.3708 - val_loss: 136.1645\n",
      "Epoch 90/600\n",
      "34/34 [==============================] - 1s 44ms/step - loss: 149.6762 - val_loss: 137.2781\n",
      "Epoch 91/600\n",
      "34/34 [==============================] - 1s 43ms/step - loss: 150.0900 - val_loss: 138.1392\n",
      "Epoch 92/600\n",
      "34/34 [==============================] - 1s 43ms/step - loss: 150.3504 - val_loss: 138.2345\n",
      "Epoch 93/600\n",
      "34/34 [==============================] - 1s 44ms/step - loss: 150.7532 - val_loss: 137.0063\n",
      "Epoch 94/600\n",
      "34/34 [==============================] - 2s 44ms/step - loss: 150.8895 - val_loss: 136.1580\n",
      "Epoch 95/600\n",
      "34/34 [==============================] - 1s 43ms/step - loss: 150.7110 - val_loss: 136.4608\n",
      "Epoch 96/600\n",
      "34/34 [==============================] - 1s 42ms/step - loss: 150.3667 - val_loss: 135.7379\n",
      "Epoch 97/600\n",
      "34/34 [==============================] - 1s 42ms/step - loss: 149.8033 - val_loss: 134.6018\n",
      "Epoch 98/600\n",
      "34/34 [==============================] - 1s 42ms/step - loss: 149.4651 - val_loss: 134.3129\n",
      "Epoch 99/600\n",
      "34/34 [==============================] - 1s 43ms/step - loss: 149.1929 - val_loss: 133.8064\n",
      "Epoch 100/600\n",
      "34/34 [==============================] - 1s 43ms/step - loss: 148.7446 - val_loss: 134.2913\n",
      "Epoch 101/600\n",
      "34/34 [==============================] - 1s 43ms/step - loss: 148.6622 - val_loss: 134.1206\n",
      "Epoch 102/600\n",
      "34/34 [==============================] - 1s 43ms/step - loss: 148.5265 - val_loss: 134.1020\n",
      "Epoch 103/600\n",
      "34/34 [==============================] - 1s 43ms/step - loss: 148.4305 - val_loss: 133.8481\n",
      "Epoch 104/600\n",
      "34/34 [==============================] - 1s 43ms/step - loss: 148.1615 - val_loss: 134.0582\n",
      "Epoch 105/600\n",
      "34/34 [==============================] - 1s 43ms/step - loss: 148.1733 - val_loss: 133.9091\n",
      "Epoch 106/600\n",
      "34/34 [==============================] - 1s 44ms/step - loss: 148.0293 - val_loss: 134.1138\n",
      "Epoch 107/600\n",
      "34/34 [==============================] - 1s 43ms/step - loss: 147.8826 - val_loss: 133.7751\n",
      "Epoch 108/600\n",
      "34/34 [==============================] - 2s 44ms/step - loss: 147.7907 - val_loss: 133.8350\n",
      "Epoch 109/600\n",
      "34/34 [==============================] - 1s 43ms/step - loss: 147.7343 - val_loss: 134.1620\n",
      "Epoch 110/600\n",
      "34/34 [==============================] - 1s 43ms/step - loss: 147.6035 - val_loss: 134.0674\n",
      "Epoch 111/600\n",
      "34/34 [==============================] - 1s 43ms/step - loss: 147.4339 - val_loss: 135.0539\n",
      "Epoch 112/600\n",
      "34/34 [==============================] - 1s 42ms/step - loss: 147.4112 - val_loss: 134.3374\n",
      "Epoch 113/600\n",
      "34/34 [==============================] - 1s 43ms/step - loss: 147.3606 - val_loss: 134.2066\n",
      "Epoch 114/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 147.1205 - val_loss: 134.3514\n",
      "Epoch 115/600\n",
      "34/34 [==============================] - 1s 43ms/step - loss: 147.1248 - val_loss: 134.0115\n",
      "Epoch 116/600\n",
      "34/34 [==============================] - 1s 43ms/step - loss: 147.1305 - val_loss: 134.8344\n",
      "Epoch 117/600\n",
      "34/34 [==============================] - 1s 43ms/step - loss: 147.1765 - val_loss: 134.5315\n",
      "Epoch 118/600\n",
      "34/34 [==============================] - 1s 43ms/step - loss: 146.9575 - val_loss: 134.0187\n",
      "Epoch 119/600\n",
      "34/34 [==============================] - 1s 43ms/step - loss: 147.0394 - val_loss: 134.8788\n",
      "Epoch 120/600\n",
      "34/34 [==============================] - 1s 43ms/step - loss: 146.9976 - val_loss: 134.4555\n",
      "Epoch 121/600\n",
      "34/34 [==============================] - 1s 42ms/step - loss: 146.9511 - val_loss: 134.5541\n",
      "Epoch 122/600\n",
      "34/34 [==============================] - 1s 43ms/step - loss: 146.7345 - val_loss: 135.1024\n",
      "Epoch 123/600\n",
      "34/34 [==============================] - 1s 42ms/step - loss: 146.6790 - val_loss: 134.2958\n",
      "Epoch 124/600\n",
      "34/34 [==============================] - 1s 43ms/step - loss: 146.7378 - val_loss: 134.4119\n",
      "Epoch 125/600\n",
      "34/34 [==============================] - 1s 43ms/step - loss: 146.7241 - val_loss: 134.5348\n",
      "Epoch 126/600\n",
      "34/34 [==============================] - 1s 43ms/step - loss: 146.6240 - val_loss: 134.8545\n",
      "Epoch 127/600\n",
      "34/34 [==============================] - 1s 42ms/step - loss: 146.5561 - val_loss: 134.5120\n",
      "Epoch 128/600\n",
      "34/34 [==============================] - 1s 42ms/step - loss: 146.5639 - val_loss: 134.5121\n",
      "Epoch 129/600\n",
      "34/34 [==============================] - 1s 43ms/step - loss: 146.6035 - val_loss: 134.0658\n",
      "Epoch 130/600\n",
      "34/34 [==============================] - 1s 42ms/step - loss: 146.5192 - val_loss: 134.4457\n",
      "Epoch 131/600\n",
      "34/34 [==============================] - 1s 42ms/step - loss: 146.4038 - val_loss: 135.1496\n",
      "Epoch 132/600\n",
      "34/34 [==============================] - 1s 42ms/step - loss: 146.3382 - val_loss: 135.5483\n",
      "Epoch 133/600\n",
      "34/34 [==============================] - 1s 43ms/step - loss: 146.2289 - val_loss: 134.7906\n",
      "Epoch 134/600\n",
      "34/34 [==============================] - 1s 43ms/step - loss: 146.0894 - val_loss: 135.8072\n",
      "Epoch 135/600\n",
      "34/34 [==============================] - 2s 47ms/step - loss: 146.2353 - val_loss: 135.4010\n",
      "Epoch 136/600\n",
      "34/34 [==============================] - 1s 44ms/step - loss: 146.1467 - val_loss: 136.2479\n",
      "Epoch 137/600\n",
      "34/34 [==============================] - 2s 45ms/step - loss: 146.2748 - val_loss: 136.3566\n",
      "Epoch 138/600\n",
      "34/34 [==============================] - 2s 47ms/step - loss: 146.3802 - val_loss: 135.8375\n",
      "Epoch 139/600\n",
      "34/34 [==============================] - 1s 44ms/step - loss: 146.4157 - val_loss: 135.9337\n",
      "Epoch 140/600\n",
      "34/34 [==============================] - 1s 44ms/step - loss: 146.1987 - val_loss: 135.6012\n",
      "Epoch 141/600\n",
      "34/34 [==============================] - 1s 42ms/step - loss: 146.1422 - val_loss: 135.9419\n",
      "Epoch 142/600\n",
      "34/34 [==============================] - 1s 43ms/step - loss: 146.2373 - val_loss: 135.9483\n",
      "Epoch 143/600\n",
      "34/34 [==============================] - 1s 43ms/step - loss: 146.0995 - val_loss: 134.8925\n",
      "Epoch 144/600\n",
      "34/34 [==============================] - 1s 43ms/step - loss: 146.1627 - val_loss: 134.7436\n",
      "Epoch 145/600\n",
      "34/34 [==============================] - 1s 42ms/step - loss: 145.9768 - val_loss: 135.7968\n",
      "Epoch 146/600\n",
      "34/34 [==============================] - 1s 43ms/step - loss: 146.1534 - val_loss: 135.8653\n",
      "Epoch 147/600\n",
      "34/34 [==============================] - 1s 42ms/step - loss: 146.0682 - val_loss: 136.0443\n",
      "Epoch 148/600\n",
      "34/34 [==============================] - 1s 43ms/step - loss: 146.0625 - val_loss: 136.3753\n",
      "Epoch 149/600\n",
      "34/34 [==============================] - 1s 42ms/step - loss: 146.3361 - val_loss: 136.1194\n",
      "Epoch 150/600\n",
      "34/34 [==============================] - 1s 42ms/step - loss: 146.5520 - val_loss: 136.1011\n",
      "Epoch 151/600\n",
      "34/34 [==============================] - 1s 42ms/step - loss: 146.5306 - val_loss: 135.5692\n",
      "Epoch 152/600\n",
      "34/34 [==============================] - 1s 42ms/step - loss: 146.6893 - val_loss: 137.1896\n",
      "Epoch 153/600\n",
      "34/34 [==============================] - 1s 42ms/step - loss: 147.5535 - val_loss: 137.2108\n",
      "Epoch 154/600\n",
      "34/34 [==============================] - 1s 42ms/step - loss: 147.5728 - val_loss: 137.6411\n",
      "Epoch 155/600\n",
      "34/34 [==============================] - 1s 43ms/step - loss: 147.4922 - val_loss: 135.8203\n",
      "34/34 [==============================] - 1s 44ms/step - loss: 147.6828 - val_loss: 135.7719\n",
      "Epoch 157/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 146.9720 - val_loss: 135.8134\n",
      "Epoch 158/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 147.0724 - val_loss: 134.6013\n",
      "Epoch 159/600\n",
      "34/34 [==============================] - 1s 43ms/step - loss: 147.1313 - val_loss: 135.1867\n",
      "Epoch 160/600\n",
      "34/34 [==============================] - 2s 45ms/step - loss: 146.7636 - val_loss: 134.9753\n",
      "Epoch 161/600\n",
      "34/34 [==============================] - 1s 43ms/step - loss: 146.4711 - val_loss: 134.7352\n",
      "Epoch 162/600\n",
      "34/34 [==============================] - 1s 42ms/step - loss: 146.2915 - val_loss: 134.1578\n",
      "Epoch 163/600\n",
      "34/34 [==============================] - 1s 42ms/step - loss: 146.2960 - val_loss: 135.5761\n",
      "Epoch 164/600\n",
      "34/34 [==============================] - 1s 43ms/step - loss: 146.3947 - val_loss: 135.9062\n",
      "Epoch 165/600\n",
      "34/34 [==============================] - 1s 44ms/step - loss: 146.5375 - val_loss: 135.6568\n",
      "Epoch 166/600\n",
      "34/34 [==============================] - 1s 44ms/step - loss: 146.2053 - val_loss: 134.9943\n",
      "Epoch 167/600\n",
      "34/34 [==============================] - 1s 43ms/step - loss: 146.5168 - val_loss: 134.9169\n",
      "Epoch 168/600\n",
      "34/34 [==============================] - 1s 43ms/step - loss: 146.2641 - val_loss: 135.5483\n",
      "Epoch 169/600\n",
      "34/34 [==============================] - 1s 42ms/step - loss: 146.3167 - val_loss: 134.6131\n",
      "Epoch 170/600\n",
      "34/34 [==============================] - 1s 42ms/step - loss: 146.5993 - val_loss: 135.3062\n",
      "Epoch 171/600\n",
      "34/34 [==============================] - 1s 42ms/step - loss: 146.6814 - val_loss: 134.8348\n",
      "Epoch 172/600\n",
      "34/34 [==============================] - 1s 42ms/step - loss: 146.5308 - val_loss: 134.7778\n",
      "Epoch 173/600\n",
      "34/34 [==============================] - 1s 43ms/step - loss: 146.4735 - val_loss: 135.4662\n",
      "Epoch 174/600\n",
      "34/34 [==============================] - 1s 43ms/step - loss: 146.8892 - val_loss: 135.2315\n",
      "Epoch 175/600\n",
      "34/34 [==============================] - 1s 44ms/step - loss: 147.3326 - val_loss: 134.1582\n",
      "Epoch 176/600\n",
      "34/34 [==============================] - 1s 42ms/step - loss: 146.8464 - val_loss: 134.4517\n",
      "Epoch 177/600\n",
      "34/34 [==============================] - 1s 42ms/step - loss: 146.2581 - val_loss: 134.2401\n",
      "Epoch 178/600\n",
      "34/34 [==============================] - 1s 42ms/step - loss: 145.8193 - val_loss: 134.6712\n",
      "Epoch 179/600\n",
      "34/34 [==============================] - 1s 42ms/step - loss: 145.2207 - val_loss: 134.4761\n",
      "Epoch 180/600\n",
      "34/34 [==============================] - 1s 43ms/step - loss: 145.2198 - val_loss: 134.5837\n",
      "Epoch 181/600\n",
      "34/34 [==============================] - 1s 43ms/step - loss: 145.1541 - val_loss: 135.3448\n",
      "Epoch 182/600\n",
      "34/34 [==============================] - 1s 42ms/step - loss: 144.9390 - val_loss: 134.8759\n",
      "Epoch 183/600\n",
      "34/34 [==============================] - 1s 43ms/step - loss: 144.8677 - val_loss: 135.3060\n",
      "Epoch 184/600\n",
      "34/34 [==============================] - 1s 44ms/step - loss: 144.7729 - val_loss: 135.1714\n",
      "Epoch 185/600\n",
      "34/34 [==============================] - 2s 47ms/step - loss: 144.8152 - val_loss: 135.3699\n",
      "Epoch 186/600\n",
      "34/34 [==============================] - 2s 49ms/step - loss: 144.5421 - val_loss: 135.6850\n",
      "Epoch 187/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 144.5396 - val_loss: 135.3119\n",
      "Epoch 188/600\n",
      "34/34 [==============================] - 2s 45ms/step - loss: 144.4535 - val_loss: 136.5559\n",
      "Epoch 189/600\n",
      "34/34 [==============================] - 2s 45ms/step - loss: 144.5904 - val_loss: 135.6797\n",
      "Epoch 190/600\n",
      "34/34 [==============================] - 2s 44ms/step - loss: 144.3484 - val_loss: 135.1144\n",
      "Epoch 191/600\n",
      "34/34 [==============================] - 2s 44ms/step - loss: 144.5467 - val_loss: 134.6234\n",
      "Epoch 192/600\n",
      "34/34 [==============================] - 2s 45ms/step - loss: 144.3668 - val_loss: 134.7827\n",
      "Epoch 193/600\n",
      "34/34 [==============================] - 2s 45ms/step - loss: 144.4539 - val_loss: 134.6865\n",
      "Epoch 194/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 144.5466 - val_loss: 135.7621\n",
      "Epoch 195/600\n",
      "34/34 [==============================] - 2s 44ms/step - loss: 144.5341 - val_loss: 135.5948\n",
      "Epoch 196/600\n",
      "34/34 [==============================] - 2s 44ms/step - loss: 144.5403 - val_loss: 135.0580\n",
      "Epoch 197/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 144.3948 - val_loss: 136.4899\n",
      "Epoch 198/600\n",
      "34/34 [==============================] - 2s 45ms/step - loss: 144.4896 - val_loss: 135.7660\n",
      "Epoch 199/600\n",
      "34/34 [==============================] - 2s 45ms/step - loss: 144.4548 - val_loss: 135.9334\n",
      "Epoch 200/600\n",
      "34/34 [==============================] - 2s 45ms/step - loss: 144.6068 - val_loss: 134.6177\n",
      "Epoch 201/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 144.5601 - val_loss: 134.9823\n",
      "Epoch 202/600\n",
      "34/34 [==============================] - 1s 43ms/step - loss: 144.3723 - val_loss: 134.4446\n",
      "Epoch 203/600\n",
      "34/34 [==============================] - 1s 43ms/step - loss: 144.4205 - val_loss: 134.5595\n",
      "Epoch 204/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 144.5790 - val_loss: 133.7298\n",
      "Epoch 205/600\n",
      "34/34 [==============================] - 2s 48ms/step - loss: 144.6660 - val_loss: 134.5581\n",
      "Epoch 206/600\n",
      "34/34 [==============================] - 2s 48ms/step - loss: 145.0567 - val_loss: 133.8724\n",
      "Epoch 207/600\n",
      "34/34 [==============================] - 2s 48ms/step - loss: 145.4897 - val_loss: 134.3089\n",
      "Epoch 208/600\n",
      "34/34 [==============================] - 1s 43ms/step - loss: 146.1739 - val_loss: 133.9886\n",
      "Epoch 209/600\n",
      "34/34 [==============================] - 2s 44ms/step - loss: 146.3205 - val_loss: 134.8599\n",
      "Epoch 210/600\n",
      "34/34 [==============================] - 2s 47ms/step - loss: 146.6813 - val_loss: 136.4124\n",
      "Epoch 211/600\n",
      "34/34 [==============================] - 2s 47ms/step - loss: 144.8540 - val_loss: 136.0217\n",
      "Epoch 212/600\n",
      "34/34 [==============================] - 2s 48ms/step - loss: 144.3648 - val_loss: 135.8691\n",
      "Epoch 213/600\n",
      "34/34 [==============================] - 1s 43ms/step - loss: 143.9125 - val_loss: 135.3004\n",
      "Epoch 214/600\n",
      "34/34 [==============================] - 2s 47ms/step - loss: 143.6579 - val_loss: 134.8514\n",
      "Epoch 215/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 143.5590 - val_loss: 134.4572\n",
      "Epoch 216/600\n",
      "34/34 [==============================] - 1s 42ms/step - loss: 143.6574 - val_loss: 135.5940\n",
      "Epoch 217/600\n",
      "34/34 [==============================] - 1s 44ms/step - loss: 143.4222 - val_loss: 135.3681\n",
      "Epoch 218/600\n",
      "34/34 [==============================] - 1s 44ms/step - loss: 143.2499 - val_loss: 135.4474\n",
      "Epoch 219/600\n",
      "34/34 [==============================] - 1s 43ms/step - loss: 143.2509 - val_loss: 135.5825\n",
      "Epoch 220/600\n",
      "34/34 [==============================] - 2s 47ms/step - loss: 143.2251 - val_loss: 135.9847\n",
      "Epoch 221/600\n",
      "34/34 [==============================] - 2s 45ms/step - loss: 143.2830 - val_loss: 136.5602\n",
      "Epoch 222/600\n",
      "34/34 [==============================] - 1s 42ms/step - loss: 143.0344 - val_loss: 136.1859\n",
      "Epoch 223/600\n",
      "34/34 [==============================] - 1s 43ms/step - loss: 142.9773 - val_loss: 136.5076\n",
      "Epoch 224/600\n",
      "34/34 [==============================] - 1s 44ms/step - loss: 142.8994 - val_loss: 136.0695\n",
      "Epoch 225/600\n",
      "34/34 [==============================] - 1s 42ms/step - loss: 142.7522 - val_loss: 136.2886\n",
      "Epoch 226/600\n",
      "34/34 [==============================] - 1s 43ms/step - loss: 142.6130 - val_loss: 135.9650\n",
      "Epoch 227/600\n",
      "34/34 [==============================] - 1s 43ms/step - loss: 142.5688 - val_loss: 135.2283\n",
      "Epoch 228/600\n",
      "34/34 [==============================] - 1s 42ms/step - loss: 142.3855 - val_loss: 135.1360\n",
      "Epoch 229/600\n",
      "34/34 [==============================] - 2s 45ms/step - loss: 142.3513 - val_loss: 133.9171\n",
      "Epoch 230/600\n",
      "34/34 [==============================] - 1s 44ms/step - loss: 142.4714 - val_loss: 134.2395\n",
      "Epoch 231/600\n",
      "34/34 [==============================] - 1s 42ms/step - loss: 142.4052 - val_loss: 134.4562\n",
      "Epoch 232/600\n",
      "34/34 [==============================] - 1s 43ms/step - loss: 142.2832 - val_loss: 135.1044\n",
      "Epoch 233/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34/34 [==============================] - 1s 42ms/step - loss: 142.3853 - val_loss: 134.1011\n",
      "Epoch 234/600\n",
      "34/34 [==============================] - 2s 44ms/step - loss: 142.3228 - val_loss: 134.8532\n",
      "Epoch 235/600\n",
      "34/34 [==============================] - 1s 44ms/step - loss: 142.2588 - val_loss: 134.8331\n",
      "Epoch 236/600\n",
      "34/34 [==============================] - 1s 43ms/step - loss: 142.6255 - val_loss: 134.5482\n",
      "Epoch 237/600\n",
      "34/34 [==============================] - 1s 43ms/step - loss: 142.5364 - val_loss: 134.9104\n",
      "Epoch 238/600\n",
      "34/34 [==============================] - 2s 44ms/step - loss: 142.6134 - val_loss: 134.7059\n",
      "Epoch 239/600\n",
      "34/34 [==============================] - 2s 47ms/step - loss: 142.6284 - val_loss: 134.9820\n",
      "Epoch 240/600\n",
      "34/34 [==============================] - 2s 48ms/step - loss: 142.7501 - val_loss: 135.5601\n",
      "Epoch 241/600\n",
      "34/34 [==============================] - 2s 50ms/step - loss: 142.8515 - val_loss: 135.3673\n",
      "Epoch 242/600\n",
      "34/34 [==============================] - 2s 47ms/step - loss: 142.7381 - val_loss: 135.5410\n",
      "Epoch 243/600\n",
      "34/34 [==============================] - 2s 44ms/step - loss: 142.8294 - val_loss: 136.5202\n",
      "Epoch 244/600\n",
      "34/34 [==============================] - 2s 47ms/step - loss: 142.5131 - val_loss: 135.9022\n",
      "Epoch 245/600\n",
      "34/34 [==============================] - 1s 44ms/step - loss: 142.5297 - val_loss: 134.4347\n",
      "Epoch 246/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 142.2861 - val_loss: 134.7914\n",
      "Epoch 247/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 142.4466 - val_loss: 133.8967\n",
      "Epoch 248/600\n",
      "34/34 [==============================] - 1s 44ms/step - loss: 142.3344 - val_loss: 135.3045\n",
      "Epoch 249/600\n",
      "34/34 [==============================] - 1s 42ms/step - loss: 142.2265 - val_loss: 135.4135\n",
      "Epoch 250/600\n",
      "34/34 [==============================] - 1s 42ms/step - loss: 142.2546 - val_loss: 135.9919\n",
      "Epoch 251/600\n",
      "34/34 [==============================] - 1s 43ms/step - loss: 142.3789 - val_loss: 135.5265\n",
      "Epoch 252/600\n",
      "34/34 [==============================] - 1s 43ms/step - loss: 142.2035 - val_loss: 136.2489\n",
      "Epoch 253/600\n",
      "34/34 [==============================] - 1s 43ms/step - loss: 142.6299 - val_loss: 136.7372\n",
      "Epoch 254/600\n",
      "34/34 [==============================] - 1s 43ms/step - loss: 142.4244 - val_loss: 136.3056\n",
      "Epoch 255/600\n",
      "34/34 [==============================] - 2s 44ms/step - loss: 142.5658 - val_loss: 136.7696\n",
      "Epoch 256/600\n",
      "34/34 [==============================] - 2s 44ms/step - loss: 142.3469 - val_loss: 136.9115\n",
      "Epoch 257/600\n",
      "34/34 [==============================] - 2s 47ms/step - loss: 142.1947 - val_loss: 137.0952\n",
      "Epoch 258/600\n",
      "34/34 [==============================] - 1s 44ms/step - loss: 142.1926 - val_loss: 137.1383\n",
      "Epoch 259/600\n",
      "34/34 [==============================] - 2s 45ms/step - loss: 142.2429 - val_loss: 138.1434\n",
      "Epoch 260/600\n",
      "34/34 [==============================] - 1s 43ms/step - loss: 142.1464 - val_loss: 138.6059\n",
      "Epoch 261/600\n",
      "34/34 [==============================] - 1s 43ms/step - loss: 142.0851 - val_loss: 139.1481\n",
      "Epoch 262/600\n",
      "34/34 [==============================] - 1s 44ms/step - loss: 142.1269 - val_loss: 140.3260\n",
      "Epoch 263/600\n",
      "34/34 [==============================] - 1s 44ms/step - loss: 142.1768 - val_loss: 139.5362\n",
      "Epoch 264/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 142.1840 - val_loss: 137.4183\n",
      "Epoch 265/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 142.0324 - val_loss: 137.1342\n",
      "Epoch 266/600\n",
      "34/34 [==============================] - 1s 43ms/step - loss: 141.8350 - val_loss: 137.1655\n",
      "Epoch 267/600\n",
      "34/34 [==============================] - 1s 43ms/step - loss: 141.6964 - val_loss: 136.4094\n",
      "Epoch 268/600\n",
      "34/34 [==============================] - 1s 43ms/step - loss: 141.6750 - val_loss: 138.3994\n",
      "Epoch 269/600\n",
      "34/34 [==============================] - 1s 44ms/step - loss: 141.7526 - val_loss: 137.2778\n",
      "Epoch 270/600\n",
      "34/34 [==============================] - 1s 44ms/step - loss: 141.6044 - val_loss: 136.8807\n",
      "Epoch 271/600\n",
      "34/34 [==============================] - 1s 44ms/step - loss: 141.7451 - val_loss: 136.2484\n",
      "Epoch 272/600\n",
      "34/34 [==============================] - 1s 44ms/step - loss: 141.4683 - val_loss: 136.5553\n",
      "Epoch 273/600\n",
      "34/34 [==============================] - 1s 43ms/step - loss: 141.5822 - val_loss: 136.0675\n",
      "Epoch 274/600\n",
      "34/34 [==============================] - 1s 43ms/step - loss: 141.4311 - val_loss: 137.1347\n",
      "Epoch 275/600\n",
      "34/34 [==============================] - 1s 43ms/step - loss: 141.3773 - val_loss: 137.1355\n",
      "Epoch 276/600\n",
      "34/34 [==============================] - 1s 43ms/step - loss: 141.4384 - val_loss: 136.9316\n",
      "Epoch 277/600\n",
      "34/34 [==============================] - 1s 43ms/step - loss: 141.4659 - val_loss: 137.7078\n",
      "Epoch 278/600\n",
      "34/34 [==============================] - 1s 44ms/step - loss: 141.4812 - val_loss: 137.8552\n",
      "Epoch 279/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 141.1809 - val_loss: 138.5090\n",
      "Epoch 280/600\n",
      "34/34 [==============================] - 1s 44ms/step - loss: 140.9745 - val_loss: 138.5774\n",
      "Epoch 281/600\n",
      "34/34 [==============================] - 2s 47ms/step - loss: 140.7342 - val_loss: 138.3969\n",
      "Epoch 282/600\n",
      "34/34 [==============================] - 2s 50ms/step - loss: 140.7291 - val_loss: 137.0562\n",
      "Epoch 283/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 140.5262 - val_loss: 137.2404\n",
      "Epoch 284/600\n",
      "34/34 [==============================] - 2s 47ms/step - loss: 140.5480 - val_loss: 137.0170\n",
      "Epoch 285/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 140.6632 - val_loss: 136.3454\n",
      "Epoch 286/600\n",
      "34/34 [==============================] - 1s 43ms/step - loss: 140.9470 - val_loss: 136.5881\n",
      "Epoch 287/600\n",
      "34/34 [==============================] - 1s 43ms/step - loss: 140.6622 - val_loss: 135.8255\n",
      "Epoch 288/600\n",
      "34/34 [==============================] - 1s 43ms/step - loss: 140.6113 - val_loss: 135.6260\n",
      "Epoch 289/600\n",
      "34/34 [==============================] - 2s 44ms/step - loss: 140.7728 - val_loss: 135.1092\n",
      "Epoch 290/600\n",
      "34/34 [==============================] - 2s 45ms/step - loss: 140.9979 - val_loss: 135.6145\n",
      "Epoch 291/600\n",
      "34/34 [==============================] - 1s 44ms/step - loss: 141.2364 - val_loss: 136.0707\n",
      "Epoch 292/600\n",
      "34/34 [==============================] - 1s 43ms/step - loss: 141.3742 - val_loss: 135.9652\n",
      "Epoch 293/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 141.0346 - val_loss: 136.1223\n",
      "Epoch 294/600\n",
      "34/34 [==============================] - 2s 49ms/step - loss: 141.2858 - val_loss: 137.3034\n",
      "Epoch 295/600\n",
      "34/34 [==============================] - 1s 43ms/step - loss: 141.5152 - val_loss: 137.9517\n",
      "Epoch 296/600\n",
      "34/34 [==============================] - 1s 43ms/step - loss: 141.2948 - val_loss: 136.7206\n",
      "Epoch 297/600\n",
      "34/34 [==============================] - 1s 43ms/step - loss: 141.1164 - val_loss: 135.7426\n",
      "Epoch 298/600\n",
      "34/34 [==============================] - 1s 44ms/step - loss: 141.0428 - val_loss: 136.2638\n",
      "Epoch 299/600\n",
      "34/34 [==============================] - 1s 43ms/step - loss: 140.8684 - val_loss: 135.6482\n",
      "Epoch 300/600\n",
      "34/34 [==============================] - 1s 43ms/step - loss: 140.9512 - val_loss: 136.5296\n",
      "Epoch 301/600\n",
      "34/34 [==============================] - 1s 43ms/step - loss: 140.6280 - val_loss: 135.6373\n",
      "Epoch 302/600\n",
      "34/34 [==============================] - 1s 42ms/step - loss: 140.5882 - val_loss: 135.1580\n",
      "Epoch 303/600\n",
      "34/34 [==============================] - 1s 42ms/step - loss: 140.4347 - val_loss: 135.6350\n",
      "Epoch 304/600\n",
      "34/34 [==============================] - 2s 45ms/step - loss: 140.6369 - val_loss: 135.4333\n",
      "Epoch 305/600\n",
      "34/34 [==============================] - 1s 43ms/step - loss: 140.9272 - val_loss: 135.3534\n",
      "Epoch 306/600\n",
      "34/34 [==============================] - 2s 44ms/step - loss: 140.7827 - val_loss: 135.3624\n",
      "Epoch 307/600\n",
      "34/34 [==============================] - 1s 44ms/step - loss: 141.4008 - val_loss: 136.2764\n",
      "Epoch 308/600\n",
      "34/34 [==============================] - 2s 45ms/step - loss: 141.1808 - val_loss: 136.6674\n",
      "Epoch 309/600\n",
      "34/34 [==============================] - 2s 45ms/step - loss: 141.0001 - val_loss: 135.8523\n",
      "34/34 [==============================] - 2s 45ms/step - loss: 140.9790 - val_loss: 136.9753\n",
      "Epoch 311/600\n",
      "34/34 [==============================] - 1s 44ms/step - loss: 140.7408 - val_loss: 136.3946\n",
      "Epoch 312/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 140.9743 - val_loss: 136.8099\n",
      "Epoch 313/600\n",
      "34/34 [==============================] - 2s 45ms/step - loss: 140.6608 - val_loss: 138.3266\n",
      "Epoch 314/600\n",
      "34/34 [==============================] - 2s 47ms/step - loss: 140.6510 - val_loss: 136.9312\n",
      "Epoch 315/600\n",
      "34/34 [==============================] - 2s 49ms/step - loss: 140.5007 - val_loss: 138.0331\n",
      "Epoch 316/600\n",
      "34/34 [==============================] - 2s 49ms/step - loss: 140.8078 - val_loss: 136.3254\n",
      "Epoch 317/600\n",
      "34/34 [==============================] - 2s 48ms/step - loss: 140.6286 - val_loss: 136.7110\n",
      "Epoch 318/600\n",
      "34/34 [==============================] - 2s 49ms/step - loss: 140.6951 - val_loss: 136.8296\n",
      "Epoch 319/600\n",
      "34/34 [==============================] - 2s 49ms/step - loss: 141.0117 - val_loss: 136.2443\n",
      "Epoch 320/600\n",
      "34/34 [==============================] - 2s 48ms/step - loss: 140.8298 - val_loss: 136.9564\n",
      "Epoch 321/600\n",
      "34/34 [==============================] - 2s 50ms/step - loss: 140.7996 - val_loss: 135.6990\n",
      "Epoch 322/600\n",
      "34/34 [==============================] - 2s 50ms/step - loss: 141.0951 - val_loss: 135.5361\n",
      "Epoch 323/600\n",
      "34/34 [==============================] - 2s 49ms/step - loss: 141.1717 - val_loss: 136.6288\n",
      "Epoch 324/600\n",
      "34/34 [==============================] - 2s 47ms/step - loss: 140.8041 - val_loss: 135.4642\n",
      "Epoch 325/600\n",
      "34/34 [==============================] - 1s 44ms/step - loss: 140.4826 - val_loss: 135.3171\n",
      "Epoch 326/600\n",
      "34/34 [==============================] - 2s 44ms/step - loss: 140.2942 - val_loss: 136.8911\n",
      "Epoch 327/600\n",
      "34/34 [==============================] - 1s 44ms/step - loss: 140.3775 - val_loss: 136.5200\n",
      "Epoch 328/600\n",
      "34/34 [==============================] - 2s 45ms/step - loss: 140.3965 - val_loss: 137.6756\n",
      "Epoch 329/600\n",
      "34/34 [==============================] - 2s 44ms/step - loss: 140.4684 - val_loss: 139.3232\n",
      "Epoch 330/600\n",
      "34/34 [==============================] - 2s 45ms/step - loss: 140.4129 - val_loss: 140.4349\n",
      "Epoch 331/600\n",
      "34/34 [==============================] - 2s 45ms/step - loss: 140.3173 - val_loss: 142.3287\n",
      "Epoch 332/600\n",
      "34/34 [==============================] - 1s 43ms/step - loss: 140.0271 - val_loss: 142.6782\n",
      "Epoch 333/600\n",
      "34/34 [==============================] - 1s 44ms/step - loss: 139.7647 - val_loss: 143.1599\n",
      "Epoch 334/600\n",
      "34/34 [==============================] - 1s 44ms/step - loss: 139.8621 - val_loss: 143.6943\n",
      "Epoch 335/600\n",
      "34/34 [==============================] - 2s 45ms/step - loss: 140.0276 - val_loss: 141.2817\n",
      "Epoch 336/600\n",
      "34/34 [==============================] - 1s 44ms/step - loss: 140.1533 - val_loss: 141.2625\n",
      "Epoch 337/600\n",
      "34/34 [==============================] - 1s 44ms/step - loss: 139.8290 - val_loss: 141.2593\n",
      "Epoch 338/600\n",
      "34/34 [==============================] - 1s 44ms/step - loss: 140.0295 - val_loss: 140.8388\n",
      "Epoch 339/600\n",
      "34/34 [==============================] - 1s 43ms/step - loss: 139.4425 - val_loss: 139.7182\n",
      "Epoch 340/600\n",
      "34/34 [==============================] - 1s 42ms/step - loss: 139.2054 - val_loss: 139.8942\n",
      "Epoch 341/600\n",
      "34/34 [==============================] - 1s 42ms/step - loss: 139.4551 - val_loss: 139.2589\n",
      "Epoch 342/600\n",
      "34/34 [==============================] - 1s 43ms/step - loss: 139.1723 - val_loss: 139.2241\n",
      "Epoch 343/600\n",
      "34/34 [==============================] - 1s 44ms/step - loss: 139.1288 - val_loss: 138.5026\n",
      "Epoch 344/600\n",
      "34/34 [==============================] - 1s 44ms/step - loss: 139.3023 - val_loss: 137.8388\n",
      "Epoch 345/600\n",
      "34/34 [==============================] - 1s 44ms/step - loss: 139.4120 - val_loss: 137.4490\n",
      "Epoch 346/600\n",
      "34/34 [==============================] - 1s 44ms/step - loss: 139.1994 - val_loss: 138.1814\n",
      "Epoch 347/600\n",
      "34/34 [==============================] - 2s 45ms/step - loss: 139.2658 - val_loss: 137.5700\n",
      "Epoch 348/600\n",
      "34/34 [==============================] - 1s 44ms/step - loss: 139.2242 - val_loss: 137.3399\n",
      "Epoch 349/600\n",
      "34/34 [==============================] - 1s 44ms/step - loss: 139.2796 - val_loss: 136.9808\n",
      "Epoch 350/600\n",
      "34/34 [==============================] - 2s 45ms/step - loss: 139.5804 - val_loss: 137.5983\n",
      "Epoch 351/600\n",
      "34/34 [==============================] - 2s 45ms/step - loss: 139.4388 - val_loss: 137.3824\n",
      "Epoch 352/600\n",
      "34/34 [==============================] - 2s 45ms/step - loss: 139.3800 - val_loss: 138.1564\n",
      "Epoch 353/600\n",
      "34/34 [==============================] - 2s 45ms/step - loss: 138.9271 - val_loss: 137.2049\n",
      "Epoch 354/600\n",
      "34/34 [==============================] - 2s 45ms/step - loss: 138.9667 - val_loss: 137.8724\n",
      "Epoch 355/600\n",
      "34/34 [==============================] - 2s 45ms/step - loss: 139.2729 - val_loss: 137.1395\n",
      "Epoch 356/600\n",
      "34/34 [==============================] - 2s 45ms/step - loss: 138.9125 - val_loss: 137.0128\n",
      "Epoch 357/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 138.8679 - val_loss: 137.0939\n",
      "Epoch 358/600\n",
      "34/34 [==============================] - 2s 45ms/step - loss: 138.6061 - val_loss: 136.8957\n",
      "Epoch 359/600\n",
      "34/34 [==============================] - 2s 45ms/step - loss: 138.6657 - val_loss: 137.2510\n",
      "Epoch 360/600\n",
      "34/34 [==============================] - 2s 45ms/step - loss: 138.3957 - val_loss: 137.3609\n",
      "Epoch 361/600\n",
      "34/34 [==============================] - 2s 45ms/step - loss: 138.3109 - val_loss: 138.3255\n",
      "Epoch 362/600\n",
      "34/34 [==============================] - 2s 45ms/step - loss: 138.0562 - val_loss: 139.0242\n",
      "Epoch 363/600\n",
      "34/34 [==============================] - 2s 45ms/step - loss: 138.1531 - val_loss: 138.8433\n",
      "Epoch 364/600\n",
      "34/34 [==============================] - 2s 45ms/step - loss: 138.7056 - val_loss: 137.7325\n",
      "Epoch 365/600\n",
      "34/34 [==============================] - 2s 45ms/step - loss: 138.8098 - val_loss: 138.7542\n",
      "Epoch 366/600\n",
      "34/34 [==============================] - 2s 45ms/step - loss: 138.3813 - val_loss: 138.2717\n",
      "Epoch 367/600\n",
      "34/34 [==============================] - 2s 45ms/step - loss: 138.4372 - val_loss: 138.5658\n",
      "Epoch 368/600\n",
      "34/34 [==============================] - 2s 45ms/step - loss: 138.3854 - val_loss: 139.0194\n",
      "Epoch 369/600\n",
      "34/34 [==============================] - 2s 45ms/step - loss: 137.9959 - val_loss: 138.2838\n",
      "Epoch 370/600\n",
      "34/34 [==============================] - 2s 45ms/step - loss: 138.0659 - val_loss: 138.8755\n",
      "Epoch 371/600\n",
      "34/34 [==============================] - 2s 45ms/step - loss: 137.7976 - val_loss: 137.4097\n",
      "Epoch 372/600\n",
      "34/34 [==============================] - 2s 45ms/step - loss: 137.9003 - val_loss: 138.7584\n",
      "Epoch 373/600\n",
      "34/34 [==============================] - 2s 48ms/step - loss: 138.0012 - val_loss: 138.6241\n",
      "Epoch 374/600\n",
      "34/34 [==============================] - 2s 45ms/step - loss: 138.2627 - val_loss: 139.0752\n",
      "Epoch 375/600\n",
      "34/34 [==============================] - 2s 45ms/step - loss: 138.5616 - val_loss: 137.4317\n",
      "Epoch 376/600\n",
      "34/34 [==============================] - 2s 45ms/step - loss: 138.6388 - val_loss: 138.0724\n",
      "Epoch 377/600\n",
      "34/34 [==============================] - 2s 45ms/step - loss: 139.3749 - val_loss: 137.7596\n",
      "Epoch 378/600\n",
      "34/34 [==============================] - 2s 45ms/step - loss: 139.4407 - val_loss: 138.9402\n",
      "Epoch 379/600\n",
      "34/34 [==============================] - 2s 45ms/step - loss: 139.6038 - val_loss: 138.3347\n",
      "Epoch 380/600\n",
      "34/34 [==============================] - 2s 45ms/step - loss: 139.5764 - val_loss: 139.0106\n",
      "Epoch 381/600\n",
      "34/34 [==============================] - 2s 45ms/step - loss: 138.8569 - val_loss: 139.6091\n",
      "Epoch 382/600\n",
      "34/34 [==============================] - 2s 45ms/step - loss: 138.5050 - val_loss: 141.2720\n",
      "Epoch 383/600\n",
      "34/34 [==============================] - 2s 45ms/step - loss: 138.3399 - val_loss: 141.8097\n",
      "Epoch 384/600\n",
      "34/34 [==============================] - 2s 45ms/step - loss: 137.9237 - val_loss: 141.5377\n",
      "Epoch 385/600\n",
      "34/34 [==============================] - 2s 45ms/step - loss: 137.4531 - val_loss: 142.5988\n",
      "Epoch 386/600\n",
      "34/34 [==============================] - 2s 45ms/step - loss: 137.2009 - val_loss: 141.3619\n",
      "Epoch 387/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34/34 [==============================] - 2s 45ms/step - loss: 137.3657 - val_loss: 142.2761\n",
      "Epoch 388/600\n",
      "34/34 [==============================] - 2s 45ms/step - loss: 137.2203 - val_loss: 140.8026\n",
      "Epoch 389/600\n",
      "34/34 [==============================] - 2s 45ms/step - loss: 137.1903 - val_loss: 141.0476\n",
      "Epoch 390/600\n",
      "34/34 [==============================] - 2s 48ms/step - loss: 137.0710 - val_loss: 140.7649\n",
      "Epoch 391/600\n",
      "34/34 [==============================] - 2s 45ms/step - loss: 137.1322 - val_loss: 141.4252\n",
      "Epoch 392/600\n",
      "34/34 [==============================] - 2s 45ms/step - loss: 137.4431 - val_loss: 141.5249\n",
      "Epoch 393/600\n",
      "34/34 [==============================] - 2s 45ms/step - loss: 137.3807 - val_loss: 141.4614\n",
      "Epoch 394/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 137.9727 - val_loss: 141.5533\n",
      "Epoch 395/600\n",
      "34/34 [==============================] - 2s 45ms/step - loss: 137.7742 - val_loss: 142.6848\n",
      "Epoch 396/600\n",
      "34/34 [==============================] - 2s 45ms/step - loss: 137.6556 - val_loss: 141.3472\n",
      "Epoch 397/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 137.5348 - val_loss: 142.2829\n",
      "Epoch 398/600\n",
      "34/34 [==============================] - 2s 45ms/step - loss: 137.8193 - val_loss: 142.4320\n",
      "Epoch 399/600\n",
      "34/34 [==============================] - 2s 44ms/step - loss: 138.1380 - val_loss: 141.2642\n",
      "Epoch 400/600\n",
      "34/34 [==============================] - 2s 45ms/step - loss: 138.0180 - val_loss: 142.6458\n",
      "Epoch 401/600\n",
      "34/34 [==============================] - 2s 45ms/step - loss: 138.1829 - val_loss: 142.9757\n",
      "Epoch 402/600\n",
      "34/34 [==============================] - 2s 45ms/step - loss: 137.9812 - val_loss: 142.9876\n",
      "Epoch 403/600\n",
      "34/34 [==============================] - 2s 45ms/step - loss: 137.8678 - val_loss: 142.8444\n",
      "Epoch 404/600\n",
      "34/34 [==============================] - 2s 45ms/step - loss: 137.4952 - val_loss: 141.6336\n",
      "Epoch 405/600\n",
      "34/34 [==============================] - 2s 45ms/step - loss: 137.2003 - val_loss: 141.5093\n",
      "Epoch 406/600\n",
      "34/34 [==============================] - 2s 45ms/step - loss: 137.0460 - val_loss: 141.6206\n",
      "Epoch 407/600\n",
      "34/34 [==============================] - 2s 45ms/step - loss: 137.0283 - val_loss: 142.0814\n",
      "Epoch 408/600\n",
      "34/34 [==============================] - 2s 45ms/step - loss: 136.9459 - val_loss: 142.0420\n",
      "Epoch 409/600\n",
      "34/34 [==============================] - 2s 45ms/step - loss: 136.8326 - val_loss: 144.1428\n",
      "Epoch 410/600\n",
      "34/34 [==============================] - 2s 45ms/step - loss: 136.7396 - val_loss: 144.3148\n",
      "Epoch 411/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 137.0055 - val_loss: 142.9268\n",
      "Epoch 412/600\n",
      "34/34 [==============================] - 2s 45ms/step - loss: 136.9211 - val_loss: 144.1188\n",
      "Epoch 413/600\n",
      "34/34 [==============================] - 2s 45ms/step - loss: 136.7937 - val_loss: 143.4787\n",
      "Epoch 414/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 136.7027 - val_loss: 145.9332\n",
      "Epoch 415/600\n",
      "34/34 [==============================] - 2s 45ms/step - loss: 136.7186 - val_loss: 144.2531\n",
      "Epoch 416/600\n",
      "34/34 [==============================] - 2s 45ms/step - loss: 137.0411 - val_loss: 143.9023\n",
      "Epoch 417/600\n",
      "34/34 [==============================] - 2s 45ms/step - loss: 137.6372 - val_loss: 141.5299\n",
      "Epoch 418/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 137.3592 - val_loss: 143.5639\n",
      "Epoch 419/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 137.6398 - val_loss: 144.2882\n",
      "Epoch 420/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 137.2957 - val_loss: 146.4559\n",
      "Epoch 421/600\n",
      "34/34 [==============================] - 2s 45ms/step - loss: 137.5155 - val_loss: 144.6556\n",
      "Epoch 422/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 137.3274 - val_loss: 144.6913\n",
      "Epoch 423/600\n",
      "34/34 [==============================] - 2s 45ms/step - loss: 136.9400 - val_loss: 143.9370\n",
      "Epoch 424/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 136.6789 - val_loss: 144.9106\n",
      "Epoch 425/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 136.4521 - val_loss: 144.1562\n",
      "Epoch 426/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 136.4735 - val_loss: 143.5854\n",
      "Epoch 427/600\n",
      "34/34 [==============================] - 2s 45ms/step - loss: 136.2401 - val_loss: 144.2756\n",
      "Epoch 428/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 136.1992 - val_loss: 144.2443\n",
      "Epoch 429/600\n",
      "34/34 [==============================] - 2s 47ms/step - loss: 136.2131 - val_loss: 145.6608\n",
      "Epoch 430/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 136.1925 - val_loss: 144.9304\n",
      "Epoch 431/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 136.4050 - val_loss: 145.0949\n",
      "Epoch 432/600\n",
      "34/34 [==============================] - 2s 45ms/step - loss: 137.4755 - val_loss: 144.4926\n",
      "Epoch 433/600\n",
      "34/34 [==============================] - 2s 45ms/step - loss: 137.7589 - val_loss: 145.5876\n",
      "Epoch 434/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 137.9856 - val_loss: 144.3925\n",
      "Epoch 435/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 137.9103 - val_loss: 144.4070\n",
      "Epoch 436/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 138.0567 - val_loss: 144.7677\n",
      "Epoch 437/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 137.1838 - val_loss: 142.9882\n",
      "Epoch 438/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 136.0871 - val_loss: 144.3652\n",
      "Epoch 439/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 135.8670 - val_loss: 144.4221\n",
      "Epoch 440/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 135.6601 - val_loss: 144.1144\n",
      "Epoch 441/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 135.7422 - val_loss: 142.7335\n",
      "Epoch 442/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 136.0942 - val_loss: 141.7747\n",
      "Epoch 443/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 136.2285 - val_loss: 141.8147\n",
      "Epoch 444/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 136.2902 - val_loss: 142.5519\n",
      "Epoch 445/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 136.1740 - val_loss: 141.5857\n",
      "Epoch 446/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 136.1300 - val_loss: 140.7974\n",
      "Epoch 447/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 135.8125 - val_loss: 140.1648\n",
      "Epoch 448/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 135.5766 - val_loss: 140.8348\n",
      "Epoch 449/600\n",
      "34/34 [==============================] - 2s 49ms/step - loss: 135.6936 - val_loss: 141.0649\n",
      "Epoch 450/600\n",
      "34/34 [==============================] - 2s 48ms/step - loss: 135.8607 - val_loss: 141.7749\n",
      "Epoch 451/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 135.9611 - val_loss: 141.9115\n",
      "Epoch 452/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 136.8343 - val_loss: 140.9707\n",
      "Epoch 453/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 137.3669 - val_loss: 140.8088\n",
      "Epoch 454/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 137.5811 - val_loss: 140.1483\n",
      "Epoch 455/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 137.9177 - val_loss: 140.5729\n",
      "Epoch 456/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 138.0336 - val_loss: 140.3970\n",
      "Epoch 457/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 137.5693 - val_loss: 140.4451\n",
      "Epoch 458/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 137.7177 - val_loss: 141.1882\n",
      "Epoch 459/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 137.6586 - val_loss: 140.8400\n",
      "Epoch 460/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 138.4143 - val_loss: 141.0095\n",
      "Epoch 461/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 137.9711 - val_loss: 140.3717\n",
      "Epoch 462/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 138.2552 - val_loss: 140.2240\n",
      "Epoch 463/600\n",
      "34/34 [==============================] - 2s 47ms/step - loss: 137.5673 - val_loss: 141.4444\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 137.5257 - val_loss: 140.2500\n",
      "Epoch 465/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 137.1848 - val_loss: 140.4661\n",
      "Epoch 466/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 137.1092 - val_loss: 141.1800\n",
      "Epoch 467/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 136.7234 - val_loss: 142.1432\n",
      "Epoch 468/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 136.3277 - val_loss: 142.8113\n",
      "Epoch 469/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 136.3014 - val_loss: 144.0887\n",
      "Epoch 470/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 135.8312 - val_loss: 144.1331\n",
      "Epoch 471/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 135.6628 - val_loss: 144.1910\n",
      "Epoch 472/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 135.4846 - val_loss: 144.2282\n",
      "Epoch 473/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 135.9704 - val_loss: 144.1233\n",
      "Epoch 474/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 135.3735 - val_loss: 147.4350\n",
      "Epoch 475/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 135.8838 - val_loss: 145.4421\n",
      "Epoch 476/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 136.2582 - val_loss: 145.6502\n",
      "Epoch 477/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 136.1581 - val_loss: 146.8468\n",
      "Epoch 478/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 135.6638 - val_loss: 147.3320\n",
      "Epoch 479/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 135.9740 - val_loss: 147.8923\n",
      "Epoch 480/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 135.9342 - val_loss: 146.9014\n",
      "Epoch 481/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 135.7469 - val_loss: 147.4988\n",
      "Epoch 482/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 135.6479 - val_loss: 148.3866\n",
      "Epoch 483/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 135.3263 - val_loss: 147.9759\n",
      "Epoch 484/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 135.1246 - val_loss: 146.1944\n",
      "Epoch 485/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 135.3060 - val_loss: 146.4290\n",
      "Epoch 486/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 134.8265 - val_loss: 145.3110\n",
      "Epoch 487/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 134.9593 - val_loss: 145.5780\n",
      "Epoch 488/600\n",
      "34/34 [==============================] - 2s 47ms/step - loss: 134.5852 - val_loss: 145.1518\n",
      "Epoch 489/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 134.8732 - val_loss: 145.6475\n",
      "Epoch 490/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 134.7973 - val_loss: 143.8884\n",
      "Epoch 491/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 134.9859 - val_loss: 143.8917\n",
      "Epoch 492/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 135.3622 - val_loss: 144.7879\n",
      "Epoch 493/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 135.1329 - val_loss: 145.0883\n",
      "Epoch 494/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 135.0123 - val_loss: 145.6436\n",
      "Epoch 495/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 134.7129 - val_loss: 145.6779\n",
      "Epoch 496/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 134.9532 - val_loss: 144.5386\n",
      "Epoch 497/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 134.6745 - val_loss: 144.5002\n",
      "Epoch 498/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 135.3211 - val_loss: 145.9825\n",
      "Epoch 499/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 135.3445 - val_loss: 146.4794\n",
      "Epoch 500/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 134.9691 - val_loss: 145.9211\n",
      "Epoch 501/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 134.8785 - val_loss: 146.3900\n",
      "Epoch 502/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 135.4226 - val_loss: 144.9985\n",
      "Epoch 503/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 135.4640 - val_loss: 146.1516\n",
      "Epoch 504/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 135.5907 - val_loss: 147.1083\n",
      "Epoch 505/600\n",
      "34/34 [==============================] - 2s 45ms/step - loss: 135.1661 - val_loss: 147.8856\n",
      "Epoch 506/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 134.9828 - val_loss: 147.0822\n",
      "Epoch 507/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 135.3715 - val_loss: 146.1877\n",
      "Epoch 508/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 135.4303 - val_loss: 146.0808\n",
      "Epoch 509/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 135.4763 - val_loss: 147.7718\n",
      "Epoch 510/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 135.6138 - val_loss: 149.5840\n",
      "Epoch 511/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 135.8148 - val_loss: 146.4570\n",
      "Epoch 512/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 136.1208 - val_loss: 145.6416\n",
      "Epoch 513/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 136.7021 - val_loss: 142.2682\n",
      "Epoch 514/600\n",
      "34/34 [==============================] - 2s 45ms/step - loss: 135.9738 - val_loss: 142.9066\n",
      "Epoch 515/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 136.9077 - val_loss: 143.3877\n",
      "Epoch 516/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 135.9884 - val_loss: 142.4059\n",
      "Epoch 517/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 135.5788 - val_loss: 143.5410\n",
      "Epoch 518/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 134.8519 - val_loss: 143.4791\n",
      "Epoch 519/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 134.4908 - val_loss: 144.5910\n",
      "Epoch 520/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 134.1789 - val_loss: 144.5958\n",
      "Epoch 521/600\n",
      "34/34 [==============================] - 2s 45ms/step - loss: 133.9238 - val_loss: 145.5065\n",
      "Epoch 522/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 133.7337 - val_loss: 146.6824\n",
      "Epoch 523/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 133.8089 - val_loss: 145.9666\n",
      "Epoch 524/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 133.8215 - val_loss: 146.0312\n",
      "Epoch 525/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 133.7289 - val_loss: 144.4341\n",
      "Epoch 526/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 133.4494 - val_loss: 144.7009\n",
      "Epoch 527/600\n",
      "34/34 [==============================] - 2s 47ms/step - loss: 133.6872 - val_loss: 144.4650\n",
      "Epoch 528/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 133.2993 - val_loss: 145.5628\n",
      "Epoch 529/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 133.4731 - val_loss: 146.2699\n",
      "Epoch 530/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 133.5691 - val_loss: 147.1108\n",
      "Epoch 531/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 133.7609 - val_loss: 148.4298\n",
      "Epoch 532/600\n",
      "34/34 [==============================] - 2s 45ms/step - loss: 133.9448 - val_loss: 148.4364\n",
      "Epoch 533/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 134.6313 - val_loss: 145.5741\n",
      "Epoch 534/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 134.8351 - val_loss: 146.2703\n",
      "Epoch 535/600\n",
      "34/34 [==============================] - 2s 45ms/step - loss: 134.4670 - val_loss: 145.8584\n",
      "Epoch 536/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 134.2649 - val_loss: 147.7898\n",
      "Epoch 537/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 133.6999 - val_loss: 147.7597\n",
      "Epoch 538/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 133.6514 - val_loss: 147.6605\n",
      "Epoch 539/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 133.3716 - val_loss: 150.1050\n",
      "Epoch 540/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 133.2607 - val_loss: 150.2529\n",
      "Epoch 541/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34/34 [==============================] - 2s 46ms/step - loss: 133.5697 - val_loss: 149.5192\n",
      "Epoch 542/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 133.4706 - val_loss: 148.3766\n",
      "Epoch 543/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 133.8055 - val_loss: 148.3919\n",
      "Epoch 544/600\n",
      "34/34 [==============================] - 2s 45ms/step - loss: 133.7280 - val_loss: 147.1725\n",
      "Epoch 545/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 133.6027 - val_loss: 147.6811\n",
      "Epoch 546/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 133.4496 - val_loss: 147.0526\n",
      "Epoch 547/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 133.6999 - val_loss: 146.1245\n",
      "Epoch 548/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 133.6330 - val_loss: 146.5956\n",
      "Epoch 549/600\n",
      "34/34 [==============================] - 2s 45ms/step - loss: 133.3718 - val_loss: 147.9286\n",
      "Epoch 550/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 133.6493 - val_loss: 149.0562\n",
      "Epoch 551/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 133.1895 - val_loss: 149.0026\n",
      "Epoch 552/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 133.5352 - val_loss: 147.2954\n",
      "Epoch 553/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 133.4619 - val_loss: 147.7376\n",
      "Epoch 554/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 133.1738 - val_loss: 146.2086\n",
      "Epoch 555/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 132.8803 - val_loss: 146.4775\n",
      "Epoch 556/600\n",
      "34/34 [==============================] - 2s 45ms/step - loss: 132.7602 - val_loss: 147.0419\n",
      "Epoch 557/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 132.8004 - val_loss: 147.7082\n",
      "Epoch 558/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 132.7452 - val_loss: 148.4892\n",
      "Epoch 559/600\n",
      "34/34 [==============================] - 2s 45ms/step - loss: 132.8828 - val_loss: 148.6580\n",
      "Epoch 560/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 133.1773 - val_loss: 148.2873\n",
      "Epoch 561/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 133.8202 - val_loss: 148.7892\n",
      "Epoch 562/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 134.1894 - val_loss: 148.3109\n",
      "Epoch 563/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 135.0070 - val_loss: 148.3951\n",
      "Epoch 564/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 134.4832 - val_loss: 146.6408\n",
      "Epoch 565/600\n",
      "34/34 [==============================] - 2s 47ms/step - loss: 134.3996 - val_loss: 148.3154\n",
      "Epoch 566/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 133.6765 - val_loss: 147.3045\n",
      "Epoch 567/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 133.9406 - val_loss: 148.5090\n",
      "Epoch 568/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 134.9962 - val_loss: 149.2270\n",
      "Epoch 569/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 134.7897 - val_loss: 150.7587\n",
      "Epoch 570/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 134.4867 - val_loss: 150.0631\n",
      "Epoch 571/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 134.7373 - val_loss: 148.6393\n",
      "Epoch 572/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 133.9860 - val_loss: 148.3361\n",
      "Epoch 573/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 133.2844 - val_loss: 150.0474\n",
      "Epoch 574/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 133.3064 - val_loss: 149.2645\n",
      "Epoch 575/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 133.0796 - val_loss: 149.6770\n",
      "Epoch 576/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 133.0362 - val_loss: 149.3986\n",
      "Epoch 577/600\n",
      "34/34 [==============================] - 2s 45ms/step - loss: 132.9999 - val_loss: 148.3777\n",
      "Epoch 578/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 133.0929 - val_loss: 148.6705\n",
      "Epoch 579/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 133.1316 - val_loss: 149.0501\n",
      "Epoch 580/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 133.2899 - val_loss: 149.8145\n",
      "Epoch 581/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 133.6173 - val_loss: 148.6873\n",
      "Epoch 582/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 133.9034 - val_loss: 148.3769\n",
      "Epoch 583/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 134.4010 - val_loss: 149.4266\n",
      "Epoch 584/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 134.4090 - val_loss: 150.5802\n",
      "Epoch 585/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 133.8262 - val_loss: 151.9098\n",
      "Epoch 586/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 133.2415 - val_loss: 151.2908\n",
      "Epoch 587/600\n",
      "34/34 [==============================] - 2s 45ms/step - loss: 133.5606 - val_loss: 152.0659\n",
      "Epoch 588/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 134.0117 - val_loss: 152.8749\n",
      "Epoch 589/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 133.8595 - val_loss: 153.4059\n",
      "Epoch 590/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 133.4200 - val_loss: 154.0671\n",
      "Epoch 591/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 133.4285 - val_loss: 151.3421\n",
      "Epoch 592/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 133.0065 - val_loss: 152.1399\n",
      "Epoch 593/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 133.3750 - val_loss: 151.7432\n",
      "Epoch 594/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 133.6496 - val_loss: 152.9641\n",
      "Epoch 595/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 133.7530 - val_loss: 153.7097\n",
      "Epoch 596/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 134.0605 - val_loss: 151.3371\n",
      "Epoch 597/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 133.3471 - val_loss: 152.9345\n",
      "Epoch 598/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 133.4139 - val_loss: 153.0237\n",
      "Epoch 599/600\n",
      "34/34 [==============================] - 2s 45ms/step - loss: 133.4027 - val_loss: 153.5298\n",
      "Epoch 600/600\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 134.0882 - val_loss: 155.1112\n"
     ]
    }
   ],
   "source": [
    "model_chk_path = 'conv_pivae_model0210.h5'\n",
    "mcp = ModelCheckpoint(model_chk_path, monitor=\"val_loss\", save_best_only=True, save_weights_only=True)\n",
    "s_n = conv_pivae.fit_generator(train_loader,\n",
    "              steps_per_epoch=len(train_x), epochs=600, \n",
    "              verbose=1,\n",
    "              validation_data = valid_loader,\n",
    "              validation_steps = len(valid_x), callbacks=[mcp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2aeee19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_pivae.load_weights(model_chk_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "15f17008",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = conv_pivae.predict([np.concatenate(train_x),\n",
    "                            np.concatenate(train_u)])\n",
    "# Outputs: post_mean, post_log_var, z_sample,fire_rate, lam_mean, lam_log_var, z_mean, z_log_var\n",
    "post_z = outputs[0]\n",
    "mean_z = outputs[6]\n",
    "labels = np.concatenate(train_u)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a3a15d",
   "metadata": {},
   "source": [
    "## Visualize the embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3f3050ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7fd8cd1b7df0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(5,5))\n",
    "ax = plt.subplot(111)\n",
    "r_ind = labels[:,1] == 1\n",
    "l_ind = labels[:,2] == 1\n",
    "ax.scatter(post_z[r_ind, 0], post_z[r_ind, 1], c=labels[r_ind,0], s=1, cmap = 'cool')\n",
    "ax.scatter(post_z[l_ind, 0], post_z[l_ind, 1], c=labels[l_ind,0], s=1, cmap = 'viridis')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a6fef1",
   "metadata": {},
   "source": [
    "## Decoding the position using Monte Carlo sampling\n",
    "\n",
    "- This method takes a while since it uses random sampling from p(z|u) to approximate p(u|x). \n",
    "- Below functions to compute marginal likelihood and decode are copied and adapted from the pi-VAE repo  https://github.com/zhd96/pi-vae/blob/main/code/util.py, https://github.com/zhd96/pi-vae/blob/main/examples/pi-vae_rat_data.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e9f52bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_marginal_lik_poisson(vae_mdl,\n",
    "                                 x_test,\n",
    "                                 u_fake,\n",
    "                                 n_sample,\n",
    "                                 log_opt=False):\n",
    "    lik_all = []\n",
    "\n",
    "    for jj in range(len(x_test)):  ## for each batch\n",
    "        lik_test = []\n",
    "        for ii in range(len(u_fake)):  ## for each unique u value\n",
    "            opts = vae_mdl.predict([x_test[jj], u_fake[ii][jj]])\n",
    "            lam_mean = opts[4]\n",
    "            lam_log_var = opts[5]\n",
    "            z_dim = lam_mean.shape\n",
    "            z_sample = np.random.normal(0,\n",
    "                                        1,\n",
    "                                        size=(n_sample, z_dim[0], z_dim[1]))\n",
    "            z_sample = z_sample * np.exp(0.5 * lam_log_var) + lam_mean\n",
    "\n",
    "            ## compute fire rate ##\n",
    "            get_fire_rate_output = K.function(\n",
    "                [vae_mdl.layers[-1].get_input_at(0)],\n",
    "                [vae_mdl.layers[-1].get_output_at(0)],\n",
    "            )\n",
    "            fire_rate = get_fire_rate_output([z_sample.reshape(-1,\n",
    "                                                               z_dim[-1])])[0]\n",
    "            fire_rate = fire_rate.reshape(n_sample, -1, fire_rate.shape[-2],\n",
    "                                          fire_rate.shape[-1])\n",
    "            ## compute p(x|z) poisson likelihood ##\n",
    "            loglik = x_test[jj] * np.log(np.clip(fire_rate, 1e-10,\n",
    "                                                 1e7)) - fire_rate\n",
    "            # n_sample*n_time*n_neuron\n",
    "            loglik = loglik.sum(axis=(-2, -1), dtype = np.float64)\n",
    "            ## sum across neurons and time\n",
    "            loglik_max = loglik.max(axis=0)\n",
    "            loglik -= loglik_max\n",
    "            if log_opt:\n",
    "                tmp = np.log(np.exp(loglik).mean(axis=0)) + (loglik_max)\n",
    "            else:\n",
    "                tmp = (np.exp(loglik).mean(axis=0)) * np.exp(loglik_max)\n",
    "            lik_test.append(tmp)\n",
    "        lik_all.append(np.array(lik_test))\n",
    "\n",
    "    return lik_all\n",
    "\n",
    "\n",
    "def decode_sampling_rat(test_x, test_y, model, sampling_num):\n",
    "    hd_bins = np.linspace(0, 1.6, 100)\n",
    "    hd_bins_dir = np.hstack([np.concatenate([np.linspace(0,1.6,100), np.linspace(0,1.6,100)])[...,None], np.zeros((200,2))]) \n",
    "    hd_bins_dir[:100][:,1]=1\n",
    "    hd_bins_dir[100:200][:,2]=1\n",
    "    nu_sample = 200\n",
    "    u_fake = []\n",
    "    for jj in range(nu_sample):\n",
    "        tmp_all = []\n",
    "        for ii in range(len(test_x)):\n",
    "            nn = test_x[ii].shape[0]\n",
    "            tmp = np.hstack((np.ones((nn, 1)) * hd_bins[jj % 100], np.zeros((nn, 2))))\n",
    "            if jj >= (nu_sample // 2):\n",
    "                tmp[:, 2] += 1\n",
    "            else:\n",
    "                tmp[:, 1] += 1\n",
    "            tmp_all.append(tmp)\n",
    "        u_fake.append(np.array(tmp_all))\n",
    "    u_fake = np.array(u_fake)\n",
    "\n",
    "    ## compute loglik\n",
    "\n",
    "    lik_all = compute_marginal_lik_poisson(model, test_x, u_fake, sampling_num)\n",
    "    decode_use = np.array(\n",
    "        [\n",
    "            (lik_all[jj]).reshape(200, -1, order=\"F\").argmax(axis=0)\n",
    "            for jj in range(len(lik_all))\n",
    "        ]\n",
    "    )\n",
    "    median_err = np.median(\n",
    "        np.abs(\n",
    "            [\n",
    "                hd_bins_dir[np.concatenate(decode_use)[i],0] - np.concatenate(test_y)[i, 0]\n",
    "                for i in range(len(np.concatenate(test_y)))\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "\n",
    "    prediction = [\n",
    "        hd_bins_dir[np.concatenate(decode_use)[i]]\n",
    "        for i in range(len(np.concatenate(test_y)))\n",
    "    ]\n",
    "\n",
    "    return median_err, prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9492d507",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_165463/761218901.py:64: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  u_fake.append(np.array(tmp_all))\n",
      "/tmp/ipykernel_165463/761218901.py:70: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  decode_use = np.array(\n"
     ]
    }
   ],
   "source": [
    "## Set smaller sampling_num to reduce computing time\n",
    "median_err, prediction = decode_sampling_rat(test_x, test_u, conv_pivae, sampling_num=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1319394f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median Err [m]: 0.09438572947425072\n"
     ]
    }
   ],
   "source": [
    "print(f'Median Err [m]: {median_err}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272d0252",
   "metadata": {},
   "source": [
    "## Decoding with the embedding using kNN\n",
    "\n",
    "- Here we test kNN decoding on piVAE embedding without label prior, the same method we use for decoding with CERBA embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e93610f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_outputs = conv_pivae.predict([np.concatenate(train_x),\n",
    "                            np.concatenate(train_u)])\n",
    "valid_outputs = conv_pivae.predict([np.concatenate(valid_x),\n",
    "                            np.concatenate(valid_u)])\n",
    "test_outputs = conv_pivae.predict([np.concatenate(test_x),\n",
    "                            np.concatenate(test_u)])\n",
    "\n",
    "train_post_z = train_outputs[6]\n",
    "valid_post_z = valid_outputs[6]\n",
    "test_post_z = test_outputs[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a47b6f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier\n",
    "import sklearn.metrics\n",
    "\n",
    "def decoding_pos_dir(emb_train, emb_valid, emb_test, label_train, label_valid, label_test):\n",
    "    metric = 'cosine'\n",
    "    neighbors = np.power(np.arange(1,6, dtype = int),2)\n",
    "    valid_scores = []\n",
    "    for n in neighbors:\n",
    "        pos_decoder = KNeighborsRegressor(n, metric = metric)\n",
    "        dir_decoder = KNeighborsClassifier(n, metric = metric)\n",
    "        pos_decoder.fit(emb_train, label_train[:,0])\n",
    "        dir_decoder.fit(emb_train, label_train[:,1])\n",
    "        pos_pred = pos_decoder.predict(emb_valid)\n",
    "        dir_pred = dir_decoder.predict(emb_valid)\n",
    "        prediction =np.stack([pos_pred, dir_pred],axis = 1)\n",
    "        valid_score = sklearn.metrics.r2_score(label_valid[:,:2], prediction)\n",
    "        valid_scores.append(valid_score)\n",
    "    \n",
    "    best_n=neighbors[np.argmax(valid_scores)]\n",
    "    \n",
    "    pos_decoder = KNeighborsRegressor(best_n, metric = metric)\n",
    "    dir_decoder = KNeighborsClassifier(n, metric = metric)\n",
    "    pos_decoder.fit(emb_train, label_train[:,0])\n",
    "    dir_decoder.fit(emb_train, label_train[:,1])\n",
    "    pos_pred = pos_decoder.predict(emb_test)\n",
    "    dir_pred = dir_decoder.predict(emb_test)\n",
    "    prediction =np.stack([pos_pred, dir_pred],axis = 1)\n",
    "    pos_test_err = np.median(abs(prediction[:,0] - label_test[:, 0]))\n",
    "    \n",
    "    return pos_test_err\n",
    "\n",
    "train_label = np.concatenate(train_u)\n",
    "valid_label = np.concatenate(valid_u)\n",
    "test_label = np.concatenate(test_u)\n",
    "pos_test_err = decoding_pos_dir(train_post_z, valid_post_z, test_post_z, train_label, valid_label, test_label )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b9269365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median Err: 0.10224993526935577 m\n"
     ]
    }
   ],
   "source": [
    "print(f'Median Err: {pos_test_err} m')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "language": "python",
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
