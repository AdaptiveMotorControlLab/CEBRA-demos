{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MMathisLab/CEBRA-demos/blob/main/Demo_hippocampus.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba5491e4",
      "metadata": {
        "id": "ba5491e4"
      },
      "source": [
        "# Encoding of space, hippocampus (CA1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2acff9e0",
      "metadata": {
        "id": "2acff9e0"
      },
      "source": [
        "In this notebook, we show how to:\n",
        "\n",
        "- use CEBRA on the hippocampus data.\n",
        "- use CEBRA with our [scikit-learn API](https://cebra.ai/docs/usage.html#quick-start-scikit-learn-api-example).\n",
        "- use the infoNCE loss for supervised and self-supervised learning within CEBRA.\n",
        "\n",
        "More specifically, this is a notebook demonstrating the information presented in [Hypothesis-driven analysis](https://cebra.ai/docs/demo_notebooks/Demo_hypothesis_testing.html), [Consistency](https://cebra.ai/docs/demo_notebooks/Demo_consistency.html), [Decoding](https://cebra.ai/docs/demo_notebooks/Demo_decoding.html) and [Topological Data Analysis](https://cebra.ai/docs/demo_notebooks/Demo_cohomology.html) into a single demo notebook. We recommend that you go through those individual notebooks for more informtion."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9da3c2c8",
      "metadata": {
        "id": "9da3c2c8",
        "outputId": "97512692-309e-414f-bf5e-fcc7395c4c6f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting cebra[datasets,integrations]\n",
            "  Downloading cebra-0.5.0rc1-py3-none-any.whl.metadata (6.0 kB)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from cebra[datasets,integrations]) (1.4.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from cebra[datasets,integrations]) (1.26.4)\n",
            "Collecting literate-dataclasses (from cebra[datasets,integrations])\n",
            "  Downloading literate_dataclasses-0.0.6-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from cebra[datasets,integrations]) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from cebra[datasets,integrations]) (1.13.1)\n",
            "Requirement already satisfied: torch>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from cebra[datasets,integrations]) (2.5.1+cu124)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from cebra[datasets,integrations]) (4.67.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from cebra[datasets,integrations]) (3.10.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from cebra[datasets,integrations]) (2.32.3)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from cebra[datasets,integrations]) (3.12.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from cebra[datasets,integrations]) (2.2.2)\n",
            "Collecting hdf5storage (from cebra[datasets,integrations])\n",
            "  Downloading hdf5storage-0.1.19-py2.py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.11/dist-packages (from cebra[datasets,integrations]) (3.1.5)\n",
            "Collecting jupyter (from cebra[datasets,integrations])\n",
            "  Downloading jupyter-1.1.1-py2.py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (from cebra[datasets,integrations]) (5.24.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->cebra[datasets,integrations]) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->cebra[datasets,integrations]) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->cebra[datasets,integrations]) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->cebra[datasets,integrations]) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->cebra[datasets,integrations]) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.4.0->cebra[datasets,integrations])\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.4.0->cebra[datasets,integrations])\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.4.0->cebra[datasets,integrations])\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.4.0->cebra[datasets,integrations])\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.4.0->cebra[datasets,integrations])\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.4.0->cebra[datasets,integrations])\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.4.0->cebra[datasets,integrations])\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.4.0->cebra[datasets,integrations])\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.4.0->cebra[datasets,integrations])\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->cebra[datasets,integrations]) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->cebra[datasets,integrations]) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.4.0->cebra[datasets,integrations])\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->cebra[datasets,integrations]) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->cebra[datasets,integrations]) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.4.0->cebra[datasets,integrations]) (1.3.0)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.11/dist-packages (from jupyter->cebra[datasets,integrations]) (6.5.5)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.11/dist-packages (from jupyter->cebra[datasets,integrations]) (6.1.0)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.11/dist-packages (from jupyter->cebra[datasets,integrations]) (7.16.6)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.11/dist-packages (from jupyter->cebra[datasets,integrations]) (6.17.1)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.11/dist-packages (from jupyter->cebra[datasets,integrations]) (7.7.1)\n",
            "Collecting jupyterlab (from jupyter->cebra[datasets,integrations])\n",
            "  Downloading jupyterlab-4.4.0b0-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->cebra[datasets,integrations]) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->cebra[datasets,integrations]) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->cebra[datasets,integrations]) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->cebra[datasets,integrations]) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->cebra[datasets,integrations]) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->cebra[datasets,integrations]) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->cebra[datasets,integrations]) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->cebra[datasets,integrations]) (2.8.2)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.11/dist-packages (from openpyxl->cebra[datasets,integrations]) (2.0.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->cebra[datasets,integrations]) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->cebra[datasets,integrations]) (2025.1)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly->cebra[datasets,integrations]) (9.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->cebra[datasets,integrations]) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->cebra[datasets,integrations]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->cebra[datasets,integrations]) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->cebra[datasets,integrations]) (2025.1.31)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->cebra[datasets,integrations]) (3.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->cebra[datasets,integrations]) (1.17.0)\n",
            "Requirement already satisfied: debugpy>=1.0 in /usr/local/lib/python3.11/dist-packages (from ipykernel->jupyter->cebra[datasets,integrations]) (1.8.0)\n",
            "Requirement already satisfied: ipython>=7.23.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel->jupyter->cebra[datasets,integrations]) (7.34.0)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.11/dist-packages (from ipykernel->jupyter->cebra[datasets,integrations]) (6.1.12)\n",
            "Requirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel->jupyter->cebra[datasets,integrations]) (0.1.7)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.11/dist-packages (from ipykernel->jupyter->cebra[datasets,integrations]) (1.6.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ipykernel->jupyter->cebra[datasets,integrations]) (5.9.5)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.11/dist-packages (from ipykernel->jupyter->cebra[datasets,integrations]) (24.0.1)\n",
            "Requirement already satisfied: tornado>=6.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel->jupyter->cebra[datasets,integrations]) (6.4.2)\n",
            "Requirement already satisfied: traitlets>=5.1.0 in /usr/local/lib/python3.11/dist-packages (from ipykernel->jupyter->cebra[datasets,integrations]) (5.7.1)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets->jupyter->cebra[datasets,integrations]) (0.2.0)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets->jupyter->cebra[datasets,integrations]) (3.6.10)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets->jupyter->cebra[datasets,integrations]) (3.0.13)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.4.0->cebra[datasets,integrations]) (3.0.2)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from jupyter-console->jupyter->cebra[datasets,integrations]) (3.0.50)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from jupyter-console->jupyter->cebra[datasets,integrations]) (2.18.0)\n",
            "Collecting async-lru>=1.0.0 (from jupyterlab->jupyter->cebra[datasets,integrations])\n",
            "  Downloading async_lru-2.0.4-py3-none-any.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: httpx>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from jupyterlab->jupyter->cebra[datasets,integrations]) (0.28.1)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.11/dist-packages (from jupyterlab->jupyter->cebra[datasets,integrations]) (5.7.2)\n",
            "Collecting jupyter-lsp>=2.0.0 (from jupyterlab->jupyter->cebra[datasets,integrations])\n",
            "  Downloading jupyter_lsp-2.2.5-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting jupyter-server<3,>=2.4.0 (from jupyterlab->jupyter->cebra[datasets,integrations])\n",
            "  Downloading jupyter_server-2.15.0-py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting jupyterlab-server<3,>=2.27.1 (from jupyterlab->jupyter->cebra[datasets,integrations])\n",
            "  Downloading jupyterlab_server-2.27.3-py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: notebook-shim>=0.2 in /usr/local/lib/python3.11/dist-packages (from jupyterlab->jupyter->cebra[datasets,integrations]) (0.2.4)\n",
            "Requirement already satisfied: setuptools>=41.1.0 in /usr/local/lib/python3.11/dist-packages (from jupyterlab->jupyter->cebra[datasets,integrations]) (75.1.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter->cebra[datasets,integrations]) (4.13.3)\n",
            "Requirement already satisfied: bleach!=5.0.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert->jupyter->cebra[datasets,integrations]) (6.2.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter->cebra[datasets,integrations]) (0.7.1)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter->cebra[datasets,integrations]) (0.3.0)\n",
            "Requirement already satisfied: mistune<4,>=2.0.3 in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter->cebra[datasets,integrations]) (3.1.2)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter->cebra[datasets,integrations]) (0.10.2)\n",
            "Requirement already satisfied: nbformat>=5.7 in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter->cebra[datasets,integrations]) (5.10.4)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter->cebra[datasets,integrations]) (1.5.1)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.11/dist-packages (from notebook->jupyter->cebra[datasets,integrations]) (23.1.0)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from notebook->jupyter->cebra[datasets,integrations]) (1.8.3)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.11/dist-packages (from notebook->jupyter->cebra[datasets,integrations]) (0.18.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.11/dist-packages (from notebook->jupyter->cebra[datasets,integrations]) (0.21.1)\n",
            "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.11/dist-packages (from notebook->jupyter->cebra[datasets,integrations]) (1.2.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert->jupyter->cebra[datasets,integrations]) (0.5.1)\n",
            "Requirement already satisfied: tinycss2<1.5,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert->jupyter->cebra[datasets,integrations]) (1.4.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.0->jupyterlab->jupyter->cebra[datasets,integrations]) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.0->jupyterlab->jupyter->cebra[datasets,integrations]) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.25.0->jupyterlab->jupyter->cebra[datasets,integrations]) (0.14.0)\n",
            "Collecting jedi>=0.16 (from ipython>=7.23.1->ipykernel->jupyter->cebra[datasets,integrations])\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel->jupyter->cebra[datasets,integrations]) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel->jupyter->cebra[datasets,integrations]) (0.7.5)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel->jupyter->cebra[datasets,integrations]) (0.2.0)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel->jupyter->cebra[datasets,integrations]) (4.9.0)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.11/dist-packages (from jupyter-core->jupyterlab->jupyter->cebra[datasets,integrations]) (4.3.6)\n",
            "Collecting jupyter-client>=6.1.12 (from ipykernel->jupyter->cebra[datasets,integrations])\n",
            "  Downloading jupyter_client-7.4.9-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting jupyter-events>=0.11.0 (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->cebra[datasets,integrations])\n",
            "  Downloading jupyter_events-0.12.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting jupyter-server-terminals>=0.4.4 (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->cebra[datasets,integrations])\n",
            "  Downloading jupyter_server_terminals-0.5.3-py3-none-any.whl.metadata (5.6 kB)\n",
            "Collecting overrides>=5.0 (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->cebra[datasets,integrations])\n",
            "  Downloading overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: websocket-client>=1.7 in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->cebra[datasets,integrations]) (1.8.0)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.11/dist-packages (from argon2-cffi->notebook->jupyter->cebra[datasets,integrations]) (21.2.0)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.11/dist-packages (from jupyter-client>=6.1.12->ipykernel->jupyter->cebra[datasets,integrations]) (0.4)\n",
            "Requirement already satisfied: babel>=2.10 in /usr/local/lib/python3.11/dist-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->cebra[datasets,integrations]) (2.17.0)\n",
            "Collecting json5>=0.9.0 (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->cebra[datasets,integrations])\n",
            "  Downloading json5-0.10.0-py3-none-any.whl.metadata (34 kB)\n",
            "Requirement already satisfied: jsonschema>=4.18.0 in /usr/local/lib/python3.11/dist-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->cebra[datasets,integrations]) (4.23.0)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.11/dist-packages (from nbformat>=5.7->nbconvert->jupyter->cebra[datasets,integrations]) (2.21.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->jupyter-console->jupyter->cebra[datasets,integrations]) (0.2.13)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.11/dist-packages (from terminado>=0.8.3->notebook->jupyter->cebra[datasets,integrations]) (0.7.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->nbconvert->jupyter->cebra[datasets,integrations]) (2.6)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.25.0->jupyterlab->jupyter->cebra[datasets,integrations]) (1.3.1)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel->jupyter->cebra[datasets,integrations]) (0.8.4)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->cebra[datasets,integrations]) (25.1.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->cebra[datasets,integrations]) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->cebra[datasets,integrations]) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->cebra[datasets,integrations]) (0.23.1)\n",
            "Collecting python-json-logger>=2.0.4 (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->cebra[datasets,integrations])\n",
            "  Downloading python_json_logger-3.2.1-py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: pyyaml>=5.3 in /usr/local/lib/python3.11/dist-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->cebra[datasets,integrations]) (6.0.2)\n",
            "Collecting rfc3339-validator (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->cebra[datasets,integrations])\n",
            "  Downloading rfc3339_validator-0.1.4-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting rfc3986-validator>=0.1.1 (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->cebra[datasets,integrations])\n",
            "  Downloading rfc3986_validator-0.1.1-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook->jupyter->cebra[datasets,integrations]) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook->jupyter->cebra[datasets,integrations]) (2.22)\n",
            "Collecting fqdn (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->cebra[datasets,integrations])\n",
            "  Downloading fqdn-1.5.1-py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting isoduration (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->cebra[datasets,integrations])\n",
            "  Downloading isoduration-20.11.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: jsonpointer>1.13 in /usr/local/lib/python3.11/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->cebra[datasets,integrations]) (3.0.0)\n",
            "Collecting uri-template (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->cebra[datasets,integrations])\n",
            "  Downloading uri_template-1.3.0-py3-none-any.whl.metadata (8.8 kB)\n",
            "Requirement already satisfied: webcolors>=24.6.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->cebra[datasets,integrations]) (24.11.1)\n",
            "Collecting arrow>=0.15.0 (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->cebra[datasets,integrations])\n",
            "  Downloading arrow-1.3.0-py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting types-python-dateutil>=2.8.10 (from arrow>=0.15.0->isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->cebra[datasets,integrations])\n",
            "  Downloading types_python_dateutil-2.9.0.20241206-py3-none-any.whl.metadata (2.1 kB)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m37.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m67.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m43.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m91.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cebra-0.5.0rc1-py3-none-any.whl (204 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m205.0/205.0 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hdf5storage-0.1.19-py2.py3-none-any.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jupyter-1.1.1-py2.py3-none-any.whl (2.7 kB)\n",
            "Downloading literate_dataclasses-0.0.6-py3-none-any.whl (5.0 kB)\n",
            "Downloading jupyterlab-4.4.0b0-py3-none-any.whl (12.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading async_lru-2.0.4-py3-none-any.whl (6.1 kB)\n",
            "Downloading jupyter_lsp-2.2.5-py3-none-any.whl (69 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.1/69.1 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jupyter_server-2.15.0-py3-none-any.whl (385 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m385.8/385.8 kB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jupyter_client-7.4.9-py3-none-any.whl (133 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.5/133.5 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jupyterlab_server-2.27.3-py3-none-any.whl (59 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.7/59.7 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m72.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading json5-0.10.0-py3-none-any.whl (34 kB)\n",
            "Downloading jupyter_events-0.12.0-py3-none-any.whl (19 kB)\n",
            "Downloading jupyter_server_terminals-0.5.3-py3-none-any.whl (13 kB)\n",
            "Downloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
            "Downloading python_json_logger-3.2.1-py3-none-any.whl (14 kB)\n",
            "Downloading rfc3986_validator-0.1.1-py2.py3-none-any.whl (4.2 kB)\n",
            "Downloading rfc3339_validator-0.1.4-py2.py3-none-any.whl (3.5 kB)\n",
            "Downloading fqdn-1.5.1-py3-none-any.whl (9.1 kB)\n",
            "Downloading isoduration-20.11.0-py3-none-any.whl (11 kB)\n",
            "Downloading uri_template-1.3.0-py3-none-any.whl (11 kB)\n",
            "Downloading arrow-1.3.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading types_python_dateutil-2.9.0.20241206-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: uri-template, types-python-dateutil, rfc3986-validator, rfc3339-validator, python-json-logger, overrides, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, literate-dataclasses, json5, jedi, fqdn, async-lru, nvidia-cusparse-cu12, nvidia-cudnn-cu12, jupyter-server-terminals, jupyter-client, hdf5storage, arrow, nvidia-cusolver-cu12, isoduration, jupyter-events, cebra, jupyter-server, jupyterlab-server, jupyter-lsp, jupyterlab, jupyter\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n"
          ]
        }
      ],
      "source": [
        "!pip install --pre 'cebra[datasets, integrations]'\n",
        "\n",
        "# later in the notebook we are going to do some topological data analysis, and we grab some precomputed results:\n",
        "!pip install ripser\n",
        "!pip install git+https://github.com/ctralie/DREiMac.git@cdd6d02ba53c3597a931db9da478fd198d6ed00f\n",
        "!wget https://github.com/AdaptiveMotorControlLab/CEBRA-demos/raw/main/rat_demo_example_output.h5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7581e3f",
      "metadata": {
        "id": "e7581e3f"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import joblib as jl\n",
        "import cebra.datasets\n",
        "from cebra import CEBRA\n",
        "\n",
        "from matplotlib.collections import LineCollection\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11734438",
      "metadata": {
        "id": "11734438"
      },
      "source": [
        "### Load the data:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "545a4435",
      "metadata": {
        "id": "545a4435"
      },
      "source": [
        "- The data will be automatically downloaded into a `/data` folder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5f8531d",
      "metadata": {
        "id": "f5f8531d"
      },
      "outputs": [],
      "source": [
        "#1. Load example data\n",
        "%mkdir data\n",
        "hippocampus_pos = cebra.datasets.init('rat-hippocampus-single-achilles')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25b87742",
      "metadata": {
        "id": "25b87742"
      },
      "source": [
        "### Visualize the data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6019b83e",
      "metadata": {
        "id": "6019b83e"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(9,3), dpi=150)\n",
        "plt.subplots_adjust(wspace = 0.3)\n",
        "ax = plt.subplot(121)\n",
        "ax.imshow(hippocampus_pos.neural.numpy()[:1000].T, aspect = 'auto', cmap = 'gray_r')\n",
        "plt.ylabel('Neuron #')\n",
        "plt.xlabel('Time [s]')\n",
        "plt.xticks(np.linspace(0, 1000, 5), np.linspace(0, 0.025*1000, 5, dtype = int))\n",
        "\n",
        "ax2 = plt.subplot(122)\n",
        "ax2.scatter(np.arange(1000), hippocampus_pos.continuous_index[:1000,0], c = 'gray', s=1)\n",
        "plt.ylabel('Position [m]')\n",
        "plt.xlabel('Time [s]')\n",
        "plt.xticks(np.linspace(0, 1000, 5), np.linspace(0, 0.025*1000, 5, dtype = int))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce4f1815",
      "metadata": {
        "id": "ce4f1815"
      },
      "source": [
        "### CEBRA-Behavior: Train a model with 3D output that uses positional information (position + direction).\n",
        "\n",
        "- Please be aware this is JUST a demo notebook; for details on reproducing our paper results, see the paper.\n",
        "- Please see our [user docs](https://cebra.ai/docs/usage.html#quick-start-scikit-learn-api-example) for our suggestions on properly using CEBRA for experiments.\n",
        "- Setting conditional = 'time_delta' means we will use CEBRA-Behavior mode and use auxiliary behavior variable for the model training."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "901b1a30",
      "metadata": {
        "id": "901b1a30"
      },
      "source": [
        "- For a quick CPU run-time demo, you can drop `max_iterations` to 100-500; otherwise, set to ~5-10000 on a GPU for a reasonable demo.\n",
        "- For a demonstration relating to Figure 2, we use output dimension 3 for the infoNCE loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2a10dbd",
      "metadata": {
        "id": "f2a10dbd"
      },
      "outputs": [],
      "source": [
        "max_iterations = 5000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "00a451d4",
      "metadata": {
        "id": "00a451d4"
      },
      "outputs": [],
      "source": [
        "#2. Define the model\n",
        "cebra_posdir3_model = CEBRA(model_architecture='offset10-model',\n",
        "                        batch_size=512,\n",
        "                        learning_rate=3e-4,\n",
        "                        temperature=1,\n",
        "                        output_dimension=3,\n",
        "                        max_iterations=max_iterations,\n",
        "                        distance='cosine',\n",
        "                        conditional='time_delta',\n",
        "                        device='cuda_if_available',\n",
        "                        verbose=True,\n",
        "                        time_offsets=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "SKqvRCOhBqfM",
      "metadata": {
        "id": "SKqvRCOhBqfM"
      },
      "source": [
        "Attention: Here we split the data for train/validation into 80/20. Note, the behavior might not be the same in the first 80% vs. last 20%, therefore, we recommend cross validating with different splitting strategies when when you use this feature! Read more at [sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_EMORvRizJBG",
      "metadata": {
        "id": "_EMORvRizJBG"
      },
      "outputs": [],
      "source": [
        "# 3. Split data and labels\n",
        "from sklearn.model_selection import train_test_split\n",
        "split_idx = int(0.8 * len(hippocampus_pos.neural)) #suggest: 5%-20% depending on your dataset size\n",
        "\n",
        "train_data = hippocampus_pos.neural[:split_idx]\n",
        "valid_data = hippocampus_pos.neural[split_idx:]\n",
        "\n",
        "train_continuous_label = hippocampus_pos.continuous_index.numpy()[:split_idx]\n",
        "valid_continuous_label = hippocampus_pos.continuous_index.numpy()[split_idx:]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "184df8b3",
      "metadata": {
        "id": "184df8b3"
      },
      "source": [
        "### We train the model with neural data and the behavior variable including position and direction.\n",
        "\n",
        "- with a V100 GPU, 5K iterations should take less then 2 min."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3923b23",
      "metadata": {
        "id": "f3923b23"
      },
      "outputs": [],
      "source": [
        "# 4. Fit the model\n",
        "cebra_posdir3_model.fit(train_data, train_continuous_label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MdSozcOZ0nmn",
      "metadata": {
        "id": "MdSozcOZ0nmn"
      },
      "outputs": [],
      "source": [
        "# 5. Save the model\n",
        "import os\n",
        "import tempfile\n",
        "from pathlib import Path\n",
        "tmp_file = Path(tempfile.gettempdir(), 'cebra.pt')\n",
        "cebra_posdir3_model.save(tmp_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "HHeBpZOC0w5g",
      "metadata": {
        "id": "HHeBpZOC0w5g"
      },
      "outputs": [],
      "source": [
        "# 6. Load the model and compute an embedding\n",
        "cebra_posdir3_model = cebra.CEBRA.load(tmp_file)\n",
        "train_embedding = cebra_posdir3_model.transform(train_data)\n",
        "valid_embedding = cebra_posdir3_model.transform(valid_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zX8gtfEf03GG",
      "metadata": {
        "id": "zX8gtfEf03GG"
      },
      "outputs": [],
      "source": [
        "# 7. Evaluate the model performance on the validation set\n",
        "goodness_of_fit = cebra.sklearn.metrics.infonce_loss(cebra_posdir3_model,\n",
        "                                                     valid_data,\n",
        "                                                     valid_continuous_label,\n",
        "                                                     num_batches=5)\n",
        "print(\" goodness of fit - validation:\",goodness_of_fit)\n",
        "\n",
        "# Evaluate the model performance on the train set\n",
        "goodness_of_fit = cebra.sklearn.metrics.infonce_loss(cebra_posdir3_model,\n",
        "                                                     train_data,\n",
        "                                                     train_continuous_label,\n",
        "                                                     num_batches=5)\n",
        "print(\" goodness of fit - train:\",goodness_of_fit)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ZE3ESNhAPbOv",
      "metadata": {
        "id": "ZE3ESNhAPbOv"
      },
      "source": [
        "### Visualize the embedding\n",
        "- Note, CEBRA's strength is quantitative measures of the resulting embedding with goodness of fit and consistency (comparisions across models, etc). But, it is useful to visualize the data for a qualitative assessment.\n",
        " - If it's collapsed or points are uniformly scattered across the sphere, either your label is not well represented in your data, or you need to adjust your model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kBmYvJcQPbmt",
      "metadata": {
        "id": "kBmYvJcQPbmt"
      },
      "outputs": [],
      "source": [
        "import cebra.integrations.plotly\n",
        "\n",
        "fig = cebra.integrations.plotly.plot_embedding_interactive(train_embedding,\n",
        "                                                           embedding_labels=train_continuous_label[:,0],\n",
        "                                                           title = \"CEBRA-Behavior Train\",\n",
        "                                                           markersize=2,\n",
        "                                                           cmap = \"rainbow\")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xC7KQUDr4Hsy",
      "metadata": {
        "id": "xC7KQUDr4Hsy"
      },
      "outputs": [],
      "source": [
        "import cebra.integrations.plotly\n",
        "\n",
        "fig = cebra.integrations.plotly.plot_embedding_interactive(valid_embedding,\n",
        "                                                           embedding_labels=valid_continuous_label[:,0],\n",
        "                                                           title = \"CEBRA-Behavior-validation\",\n",
        "                                                           markersize=2,\n",
        "                                                           cmap = \"rainbow\")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "CBte_I19_CK0",
      "metadata": {
        "id": "CBte_I19_CK0"
      },
      "source": [
        "## Shuffle Control\n",
        "- beyond train/validation splits we can compare a shuffle embedding with the same hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bAsX1IFz_b36",
      "metadata": {
        "id": "bAsX1IFz_b36"
      },
      "outputs": [],
      "source": [
        "### Shuffle the behavior variable and use it for training\n",
        "hippocampus_shuffled_posdir = np.random.permutation(hippocampus_pos.continuous_index.numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qV3tznmu_UI9",
      "metadata": {
        "id": "qV3tznmu_UI9"
      },
      "outputs": [],
      "source": [
        "# Shuffle: Split data and labels\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "(\n",
        "    shuffle_train_data,\n",
        "    shuffle_valid_data,\n",
        "    train_continuous_label,\n",
        "    valid_continuous_label,\n",
        ") = train_test_split(hippocampus_pos.neural,\n",
        "                    hippocampus_shuffled_posdir,\n",
        "                    test_size=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Ihnzal04_0GB",
      "metadata": {
        "id": "Ihnzal04_0GB"
      },
      "outputs": [],
      "source": [
        "cebra_posdir3_model.fit(shuffle_train_data, train_continuous_label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zOvPTGvP__s5",
      "metadata": {
        "id": "zOvPTGvP__s5"
      },
      "outputs": [],
      "source": [
        "shuffle_train_embedding = cebra_posdir3_model.transform(shuffle_train_data)\n",
        "shuffle_valid_embedding = cebra_posdir3_model.transform(shuffle_valid_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QycKutk6Bh-i",
      "metadata": {
        "id": "QycKutk6Bh-i"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model performance on the validation shuffle set\n",
        "goodness_of_fit = cebra.sklearn.metrics.infonce_loss(cebra_posdir3_model,\n",
        "                                                     shuffle_valid_data,\n",
        "                                                     valid_continuous_label,\n",
        "                                                     num_batches=5)\n",
        "print(\" goodness of fit - shuffle validation:\",goodness_of_fit)\n",
        "\n",
        "# Evaluate the model performance on the train shuffle set\n",
        "goodness_of_fit = cebra.sklearn.metrics.infonce_loss(cebra_posdir3_model,\n",
        "                                                     shuffle_train_data,\n",
        "                                                     train_continuous_label,\n",
        "                                                     num_batches=5)\n",
        "print(\" goodness of fit - shuffle train:\",goodness_of_fit)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "oQbTmrBUAIWg",
      "metadata": {
        "id": "oQbTmrBUAIWg"
      },
      "outputs": [],
      "source": [
        "fig = cebra.integrations.plotly.plot_embedding_interactive(shuffle_train_embedding,\n",
        "                                                           embedding_labels=train_continuous_label[:,0],\n",
        "                                                           title = \"CEBRA-Behavior Shuffle Train\",\n",
        "                                                           markersize=2,\n",
        "                                                           cmap = \"rainbow\")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "t2Qeef_EALLc",
      "metadata": {
        "id": "t2Qeef_EALLc"
      },
      "outputs": [],
      "source": [
        "fig = cebra.integrations.plotly.plot_embedding_interactive(shuffle_valid_embedding,\n",
        "                                                           embedding_labels=valid_continuous_label[:,0],\n",
        "                                                           title = \"CEBRA-Behavior Shuffle Validation\",\n",
        "                                                           markersize=2,\n",
        "                                                           cmap = \"rainbow\")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "MeOfSYHb9tZS",
      "metadata": {
        "id": "MeOfSYHb9tZS"
      },
      "source": [
        "## Using the full data\n",
        "\n",
        "- Now that we have checked our parameters are not causing overfitting, we will train on the full dataset so we can make comparisons below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2AzAYHPG90Kj",
      "metadata": {
        "id": "2AzAYHPG90Kj"
      },
      "outputs": [],
      "source": [
        "cebra_posdir3_model.fit(hippocampus_pos.neural, hippocampus_pos.continuous_index.numpy())\n",
        "cebra_posdir3 = cebra_posdir3_model.transform(hippocampus_pos.neural)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Wu2e587W-FMH",
      "metadata": {
        "id": "Wu2e587W-FMH"
      },
      "outputs": [],
      "source": [
        "fig = cebra.integrations.plotly.plot_embedding_interactive(cebra_posdir3, embedding_labels=hippocampus_pos.continuous_index[:,0], title = \"CEBRA-Behavior\", markersize=2, cmap = \"rainbow\")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8beda57b",
      "metadata": {
        "id": "8beda57b"
      },
      "source": [
        "### CEBRA-Shuffled Behavior: Train a control model with shuffled neural data.\n",
        "- The model specification is the same as the CEBRA-Behavior above.\n",
        "- Note, here we do not demonstrate the train/val split."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1828c67",
      "metadata": {
        "id": "d1828c67"
      },
      "outputs": [],
      "source": [
        "cebra_posdir_shuffled3_model = CEBRA(model_architecture='offset10-model',\n",
        "                        batch_size=512,\n",
        "                        learning_rate=3e-4,\n",
        "                        temperature=1,\n",
        "                        output_dimension=3,\n",
        "                        max_iterations=max_iterations,\n",
        "                        distance='cosine',\n",
        "                        conditional='time_delta',\n",
        "                        device='cuda_if_available',\n",
        "                        verbose=True,\n",
        "                        time_offsets=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f72a42c",
      "metadata": {
        "id": "2f72a42c"
      },
      "source": [
        "- Now we train the model with shuffled behavior variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe3705e3",
      "metadata": {
        "id": "fe3705e3"
      },
      "outputs": [],
      "source": [
        "### Shuffle the behavior variable and use it for training\n",
        "hippocampus_shuffled_posdir = np.random.permutation(hippocampus_pos.continuous_index.numpy())\n",
        "cebra_posdir_shuffled3_model.fit(hippocampus_pos.neural, hippocampus_shuffled_posdir)\n",
        "cebra_posdir_shuffled3 = cebra_posdir_shuffled3_model.transform(hippocampus_pos.neural)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33eb2b91",
      "metadata": {
        "id": "33eb2b91"
      },
      "source": [
        "### CEBRA-Time: Train a model that uses time without the behavior information.\n",
        "- We can use CEBRA -Time mode by setting conditional = 'time'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e7f2d5c",
      "metadata": {
        "id": "4e7f2d5c"
      },
      "outputs": [],
      "source": [
        "cebra_time3_model = CEBRA(model_architecture='offset10-model',\n",
        "                        batch_size=512,\n",
        "                        learning_rate=3e-4,\n",
        "                        temperature=1.12,\n",
        "                        output_dimension=3,\n",
        "                        max_iterations=max_iterations,\n",
        "                        distance='cosine',\n",
        "                        conditional='time',\n",
        "                        device='cuda_if_available',\n",
        "                        verbose=True,\n",
        "                        time_offsets=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67f6b772",
      "metadata": {
        "id": "67f6b772"
      },
      "outputs": [],
      "source": [
        "cebra_time3_model.fit(hippocampus_pos.neural)\n",
        "cebra_time3 = cebra_time3_model.transform(hippocampus_pos.neural)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19810110",
      "metadata": {
        "id": "19810110"
      },
      "source": [
        "### CEBRA-Hybrid: Train a model that uses both time and positional information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e3fac11",
      "metadata": {
        "id": "0e3fac11"
      },
      "outputs": [],
      "source": [
        "cebra_hybrid_model = CEBRA(model_architecture='offset10-model',\n",
        "                        batch_size=512,\n",
        "                        learning_rate=3e-4,\n",
        "                        temperature=1,\n",
        "                        output_dimension=3,\n",
        "                        max_iterations=max_iterations,\n",
        "                        distance='cosine',\n",
        "                        conditional='time_delta',\n",
        "                        device='cuda_if_available',\n",
        "                        verbose=True,\n",
        "                        time_offsets=10,\n",
        "                        hybrid = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3fb33bb9",
      "metadata": {
        "id": "3fb33bb9"
      },
      "outputs": [],
      "source": [
        "cebra_hybrid_model.fit(hippocampus_pos.neural, hippocampus_pos.continuous_index.numpy())\n",
        "cebra_hybrid = cebra_hybrid_model.transform(hippocampus_pos.neural)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bda4b6ae",
      "metadata": {
        "id": "bda4b6ae"
      },
      "source": [
        "### Visualize the embeddings from CEBRA-Behavior, CEBRA-Time and CEBRA-Hybrid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e834bf2f",
      "metadata": {
        "id": "e834bf2f"
      },
      "outputs": [],
      "source": [
        "def plot_hippocampus(ax, embedding, label, gray = False, idx_order = (0,1,2)):\n",
        "    r_ind = label[:,1] == 1\n",
        "    l_ind = label[:,2] == 1\n",
        "\n",
        "    if not gray:\n",
        "        r_cmap = 'cool'\n",
        "        l_cmap = 'viridis'\n",
        "        r_c = label[r_ind, 0]\n",
        "        l_c = label[l_ind, 0]\n",
        "    else:\n",
        "        r_cmap = None\n",
        "        l_cmap = None\n",
        "        r_c = 'gray'\n",
        "        l_c = 'gray'\n",
        "\n",
        "    idx1, idx2, idx3 = idx_order\n",
        "    r=ax.scatter(embedding [r_ind,idx1],\n",
        "               embedding [r_ind,idx2],\n",
        "               embedding [r_ind,idx3],\n",
        "               c=r_c,\n",
        "               cmap=r_cmap, s=0.5)\n",
        "    l=ax.scatter(embedding [l_ind,idx1],\n",
        "               embedding [l_ind,idx2],\n",
        "               embedding [l_ind,idx3],\n",
        "               c=l_c,\n",
        "               cmap=l_cmap, s=0.5)\n",
        "\n",
        "    ax.grid(False)\n",
        "    ax.xaxis.pane.fill = False\n",
        "    ax.yaxis.pane.fill = False\n",
        "    ax.zaxis.pane.fill = False\n",
        "    ax.xaxis.pane.set_edgecolor('w')\n",
        "    ax.yaxis.pane.set_edgecolor('w')\n",
        "    ax.zaxis.pane.set_edgecolor('w')\n",
        "\n",
        "    return ax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4ad4a94",
      "metadata": {
        "id": "c4ad4a94",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "fig = plt.figure(figsize=(10,2))\n",
        "\n",
        "ax1 = plt.subplot(141, projection='3d')\n",
        "ax2 = plt.subplot(142, projection='3d')\n",
        "ax3 = plt.subplot(143, projection='3d')\n",
        "ax4 = plt.subplot(144, projection='3d')\n",
        "\n",
        "ax1=plot_hippocampus(ax1, cebra_posdir3, hippocampus_pos.continuous_index)\n",
        "ax2=plot_hippocampus(ax2, cebra_posdir_shuffled3, hippocampus_pos.continuous_index)\n",
        "ax3=plot_hippocampus(ax3, cebra_time3, hippocampus_pos.continuous_index)\n",
        "ax4=plot_hippocampus(ax4, cebra_hybrid, hippocampus_pos.continuous_index)\n",
        "\n",
        "ax1.set_title('CEBRA-Behavior')\n",
        "ax2.set_title('CEBRA-Shuffled')\n",
        "ax3.set_title('CEBRA-Time')\n",
        "ax4.set_title('CEBRA-Hybrid')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "93a41a95",
      "metadata": {
        "id": "93a41a95"
      },
      "source": [
        "### Hypothesis Testing: Train models with different hypothesis on position encoding of hippocampus.\n",
        "- We will compare CEBRA-Behavior models trained with only position, only direction, both and the control models with shuffled behavior variables.\n",
        "\n",
        "- Here, we use the set model dimension; in the paper we used 3-64 on the hippocampus data (and found a consistent topology across these dimensions).\n",
        "\n",
        "- For the purpose of decoding later, we will use a splitted data (80% train, 20% test) and only use train set to train the models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zdykH-UQF99h",
      "metadata": {
        "id": "zdykH-UQF99h"
      },
      "outputs": [],
      "source": [
        "output_dimension = 32 #setting this as a hyperparameter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80af9b07",
      "metadata": {
        "id": "80af9b07"
      },
      "outputs": [],
      "source": [
        "def split_data(data, test_ratio):\n",
        "\n",
        "    split_idx = int(len(data)* (1-test_ratio))\n",
        "    neural_train = data.neural[:split_idx]\n",
        "    neural_test = data.neural[split_idx:]\n",
        "    label_train = data.continuous_index[:split_idx]\n",
        "    label_test = data.continuous_index[split_idx:]\n",
        "\n",
        "    return neural_train.numpy(), neural_test.numpy(), label_train.numpy(), label_test.numpy()\n",
        "\n",
        "neural_train, neural_test, label_train, label_test = split_data(hippocampus_pos, 0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c51fd73d",
      "metadata": {
        "id": "c51fd73d"
      },
      "outputs": [],
      "source": [
        "cebra_posdir_model = CEBRA(model_architecture='offset10-model',\n",
        "                        batch_size=512,\n",
        "                        learning_rate=3e-4,\n",
        "                        temperature=1,\n",
        "                        output_dimension=output_dimension,\n",
        "                        max_iterations=max_iterations,\n",
        "                        distance='cosine',\n",
        "                        conditional='time_delta',\n",
        "                        device='cuda_if_available',\n",
        "                        verbose=True,\n",
        "                        time_offsets=10)\n",
        "\n",
        "cebra_pos_model = CEBRA(model_architecture='offset10-model',\n",
        "                        batch_size=512,\n",
        "                        learning_rate=3e-4,\n",
        "                        temperature=1,\n",
        "                        output_dimension=output_dimension,\n",
        "                        max_iterations=max_iterations,\n",
        "                        distance='cosine',\n",
        "                        conditional='time_delta',\n",
        "                        device='cuda_if_available',\n",
        "                        verbose=True,\n",
        "                        time_offsets=10)\n",
        "\n",
        "cebra_dir_model = CEBRA(model_architecture='offset10-model',\n",
        "                        batch_size=512,\n",
        "                        learning_rate=3e-4,\n",
        "                        temperature=1,\n",
        "                        output_dimension=output_dimension,\n",
        "                        max_iterations=max_iterations,\n",
        "                        distance='cosine',\n",
        "                        device='cuda_if_available',\n",
        "                        verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b43687d2",
      "metadata": {
        "id": "b43687d2"
      },
      "outputs": [],
      "source": [
        "# Train CEBRA-Behavior models with position, direction variable or both.\n",
        "# We get train set embedding and test set embedding.\n",
        "\n",
        "cebra_posdir_model.fit(neural_train, label_train)\n",
        "cebra_posdir_train = cebra_posdir_model.transform(neural_train)\n",
        "cebra_posdir_test = cebra_posdir_model.transform(neural_test)\n",
        "\n",
        "cebra_pos_model.fit(neural_train, label_train[:,0])\n",
        "cebra_pos_train = cebra_pos_model.transform(neural_train)\n",
        "cebra_pos_test = cebra_pos_model.transform(neural_test)\n",
        "\n",
        "cebra_dir_model.fit(neural_train, label_train[:,1])\n",
        "cebra_dir_train = cebra_dir_model.transform(neural_train)\n",
        "cebra_dir_test = cebra_dir_model.transform(neural_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "efc156bb",
      "metadata": {
        "id": "efc156bb"
      },
      "source": [
        "### Train control models with shuffled behavior variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70750498",
      "metadata": {
        "id": "70750498"
      },
      "outputs": [],
      "source": [
        "cebra_posdir_shuffled_model = CEBRA(model_architecture='offset10-model',\n",
        "                        batch_size=512,\n",
        "                        learning_rate=3e-4,\n",
        "                        temperature=1,\n",
        "                        output_dimension=output_dimension,\n",
        "                        max_iterations=max_iterations,\n",
        "                        distance='cosine',\n",
        "                        conditional='time_delta',\n",
        "                        device='cuda_if_available',\n",
        "                        verbose=True,\n",
        "                        time_offsets=10)\n",
        "\n",
        "cebra_pos_shuffled_model = CEBRA(model_architecture='offset10-model',\n",
        "                        batch_size=512,\n",
        "                        learning_rate=3e-4,\n",
        "                        temperature=1,\n",
        "                        output_dimension=output_dimension,\n",
        "                        max_iterations=max_iterations,\n",
        "                        distance='cosine',\n",
        "                        conditional='time_delta',\n",
        "                        device='cuda_if_available',\n",
        "                        verbose=True,\n",
        "                        time_offsets=10)\n",
        "\n",
        "cebra_dir_shuffled_model = CEBRA(model_architecture='offset10-model',\n",
        "                        batch_size=512,\n",
        "                        learning_rate=3e-4,\n",
        "                        temperature=1,\n",
        "                        output_dimension=output_dimension,\n",
        "                        max_iterations=max_iterations,\n",
        "                        distance='cosine',\n",
        "                        device='cuda_if_available',\n",
        "                        verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01e465a3",
      "metadata": {
        "id": "01e465a3"
      },
      "outputs": [],
      "source": [
        "# Generate shuffled behavior labels for train set.\n",
        "shuffled_posdir = np.random.permutation(label_train)\n",
        "shuffled_pos = np.random.permutation(label_train[:,0])\n",
        "shuffled_dir = np.random.permutation(label_train[:,1])\n",
        "\n",
        "# Train the models with shuffled behavior variables and get train/test embeddings\n",
        "cebra_posdir_shuffled_model.fit(neural_train, shuffled_posdir)\n",
        "cebra_posdir_shuffled_train = cebra_posdir_shuffled_model.transform(neural_train)\n",
        "cebra_posdir_shuffled_test = cebra_posdir_shuffled_model.transform(neural_test)\n",
        "\n",
        "cebra_pos_shuffled_model.fit(neural_train, shuffled_pos)\n",
        "cebra_pos_shuffled_train = cebra_pos_shuffled_model.transform(neural_train)\n",
        "cebra_pos_shuffled_test = cebra_pos_shuffled_model.transform(neural_test)\n",
        "\n",
        "cebra_dir_shuffled_model.fit(neural_train, shuffled_dir)\n",
        "cebra_dir_shuffled_train = cebra_dir_shuffled_model.transform(neural_train)\n",
        "cebra_dir_shuffled_test = cebra_dir_shuffled_model.transform(neural_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15f1fafd",
      "metadata": {
        "id": "15f1fafd"
      },
      "source": [
        "### Visualize embeddings from different hypothesis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ec24f5f",
      "metadata": {
        "id": "2ec24f5f"
      },
      "outputs": [],
      "source": [
        "cebra_pos_all = cebra_pos_model.transform(hippocampus_pos.neural)\n",
        "cebra_dir_all = cebra_dir_model.transform(hippocampus_pos.neural)\n",
        "cebra_posdir_all = cebra_posdir_model.transform(hippocampus_pos.neural)\n",
        "\n",
        "cebra_pos_shuffled_all = cebra_pos_shuffled_model.transform(hippocampus_pos.neural)\n",
        "cebra_dir_shuffled_all = cebra_dir_shuffled_model.transform(hippocampus_pos.neural)\n",
        "cebra_posdir_shuffled_all = cebra_posdir_shuffled_model.transform(hippocampus_pos.neural)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2826f7a6",
      "metadata": {
        "id": "2826f7a6"
      },
      "outputs": [],
      "source": [
        "fig=plt.figure(figsize=(9,6))\n",
        "ax1=plt.subplot(231, projection = '3d')\n",
        "ax2=plt.subplot(232, projection = '3d')\n",
        "ax3=plt.subplot(233, projection = '3d')\n",
        "ax4=plt.subplot(234, projection = '3d')\n",
        "ax5=plt.subplot(235, projection = '3d')\n",
        "ax6=plt.subplot(236, projection = '3d')\n",
        "\n",
        "ax1=plot_hippocampus(ax1, cebra_pos_all, hippocampus_pos.continuous_index, gray=True)\n",
        "ax2=plot_hippocampus(ax2, cebra_dir_all, hippocampus_pos.continuous_index, gray=True)\n",
        "ax3=plot_hippocampus(ax3, cebra_posdir_all,hippocampus_pos.continuous_index, gray=True)\n",
        "ax4=plot_hippocampus(ax4, cebra_pos_shuffled_all, hippocampus_pos.continuous_index, gray=True)\n",
        "ax5=plot_hippocampus(ax5, cebra_dir_shuffled_all, hippocampus_pos.continuous_index, gray=True)\n",
        "ax6=plot_hippocampus(ax6, cebra_posdir_shuffled_all, hippocampus_pos.continuous_index, gray=True)\n",
        "\n",
        "ax1.set_title('position only')\n",
        "ax2.set_title('direction only')\n",
        "ax3.set_title('position+direction')\n",
        "ax4.set_title('position, shuffled')\n",
        "ax5.set_title('direction, shuffled')\n",
        "ax6.set_title('pos+dir, shuffled')\n",
        "\n",
        "def ax_lim(ax):\n",
        "    ax.set_xlim(-1,1)\n",
        "    ax.set_ylim(-1,1)\n",
        "    ax.set_zlim(-1,1)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e1c93053",
      "metadata": {
        "id": "e1c93053"
      },
      "source": [
        "### Visualize the loss of models trained with different hypothesis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bddcf361",
      "metadata": {
        "id": "bddcf361",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(5,5))\n",
        "ax = plt.subplot(111)\n",
        "ax.plot(cebra_posdir_model.state_dict_['loss'], c='deepskyblue', label = 'position+direction')\n",
        "ax.plot(cebra_pos_model.state_dict_['loss'], c='deepskyblue', alpha = 0.3, label = 'position')\n",
        "ax.plot(cebra_dir_model.state_dict_['loss'], c='deepskyblue', alpha=0.6,label = 'direction')\n",
        "ax.plot(cebra_posdir_shuffled_model.state_dict_['loss'], c='gray', label = 'pos+dir, shuffled')\n",
        "ax.plot(cebra_pos_shuffled_model.state_dict_['loss'], c='gray', alpha = 0.3, label = 'position, shuffled')\n",
        "ax.plot(cebra_dir_shuffled_model.state_dict_['loss'],c='gray', alpha=0.6,label = 'direction, shuffled')\n",
        "\n",
        "ax.spines['top'].set_visible(False)\n",
        "ax.spines['right'].set_visible(False)\n",
        "ax.set_xlabel('Iterations')\n",
        "ax.set_ylabel('InfoNCE Loss')\n",
        "plt.legend(bbox_to_anchor=(0.5,0.3), frameon = False )\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a921abf",
      "metadata": {
        "id": "6a921abf"
      },
      "source": [
        "### Decoding: we evaluate decoding performance of the different hypothesis models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba97a1c5",
      "metadata": {
        "id": "ba97a1c5"
      },
      "outputs": [],
      "source": [
        "# Define decoding function with kNN decoder. For a simple demo, we will use the fixed number of neighbors 36.\n",
        "\n",
        "from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier\n",
        "import sklearn.metrics\n",
        "\n",
        "def decoding_pos_dir(emb_train, emb_test, label_train, label_test, n_neighbors=36):\n",
        "    pos_decoder = KNeighborsRegressor(n_neighbors, metric = 'cosine')\n",
        "    dir_decoder = KNeighborsClassifier(n_neighbors, metric = 'cosine')\n",
        "\n",
        "    pos_decoder.fit(emb_train, label_train[:,0])\n",
        "    dir_decoder.fit(emb_train, label_train[:,1])\n",
        "\n",
        "    pos_pred = pos_decoder.predict(emb_test)\n",
        "    dir_pred = dir_decoder.predict(emb_test)\n",
        "\n",
        "    prediction =np.stack([pos_pred, dir_pred],axis = 1)\n",
        "\n",
        "    test_score = sklearn.metrics.r2_score(label_test[:,:2], prediction)\n",
        "    pos_test_err = np.median(abs(prediction[:,0] - label_test[:, 0]))\n",
        "    pos_test_score = sklearn.metrics.r2_score(label_test[:, 0], prediction[:,0])\n",
        "\n",
        "    return test_score, pos_test_err, pos_test_score"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "abe39376",
      "metadata": {
        "id": "abe39376"
      },
      "source": [
        "- Decode the position and direction from the trained hypothesis models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "389afe84",
      "metadata": {
        "id": "389afe84"
      },
      "outputs": [],
      "source": [
        "cebra_posdir_decode = decoding_pos_dir(cebra_posdir_train, cebra_posdir_test, label_train, label_test)\n",
        "cebra_pos_decode = decoding_pos_dir(cebra_pos_train, cebra_pos_test, label_train, label_test)\n",
        "cebra_dir_decode = decoding_pos_dir(cebra_dir_train, cebra_dir_test, label_train, label_test)\n",
        "cebra_posdir_shuffled_decode = decoding_pos_dir(cebra_posdir_shuffled_train, cebra_posdir_shuffled_test, label_train, label_test)\n",
        "cebra_pos_shuffled_decode = decoding_pos_dir(cebra_pos_shuffled_train, cebra_pos_shuffled_test, label_train, label_test)\n",
        "cebra_dir_shuffled_decode = decoding_pos_dir(cebra_dir_shuffled_train, cebra_dir_shuffled_test, label_train, label_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2ec095a",
      "metadata": {
        "id": "b2ec095a"
      },
      "source": [
        "### Visualize the decoding results and loss - decoding performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57890a0d",
      "metadata": {
        "id": "57890a0d"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(10,4))\n",
        "ax1= plt.subplot(121)\n",
        "ax1.bar(np.arange(6),\n",
        "        [cebra_posdir_decode[1], cebra_pos_decode[1], cebra_dir_decode[1],\n",
        "         cebra_posdir_shuffled_decode[1], cebra_pos_shuffled_decode[1], cebra_dir_shuffled_decode[1]],\n",
        "         width = 0.5, color = 'gray')\n",
        "ax1.spines['top'].set_visible(False)\n",
        "ax1.spines['right'].set_visible(False)\n",
        "ax1.set_xticks(np.arange(6))\n",
        "ax1.set_xticklabels(['pos+dir', 'pos', 'dir', 'pos+dir,\\nshuffled', 'pos,\\nshuffled', 'dir,\\nshuffled'])\n",
        "ax1.set_ylabel('Median err. [m]')\n",
        "\n",
        "ax2 = plt.subplot(122)\n",
        "ax2.scatter(cebra_posdir_model.state_dict_['loss'][-1],cebra_posdir_decode[1], s=50, c='deepskyblue', label = 'position+direction')\n",
        "ax2.scatter(cebra_pos_model.state_dict_['loss'][-1],cebra_pos_decode[1], s=50, c='deepskyblue', alpha = 0.3, label = 'position_only')\n",
        "ax2.scatter(cebra_dir_model.state_dict_['loss'][-1],cebra_dir_decode[1], s=50, c='deepskyblue', alpha=0.6,label = 'direction_only')\n",
        "ax2.scatter(cebra_posdir_shuffled_model.state_dict_['loss'][-1],cebra_posdir_shuffled_decode[1], s=50, c='gray', label = 'pos+dir, shuffled')\n",
        "ax2.scatter(cebra_pos_shuffled_model.state_dict_['loss'][-1],cebra_pos_shuffled_decode[1], s=50, c='gray', alpha = 0.3, label = 'position, shuffled')\n",
        "ax2.scatter(cebra_dir_shuffled_model.state_dict_['loss'][-1],cebra_dir_shuffled_decode[1], s=50, c='gray', alpha=0.6,label = 'direction, shuffled')\n",
        "\n",
        "ax2.spines['top'].set_visible(False)\n",
        "ax2.spines['right'].set_visible(False)\n",
        "ax2.set_xlabel('InfoNCE Loss')\n",
        "ax2.set_ylabel('Decoding Median Err.')\n",
        "plt.legend(bbox_to_anchor=(1,1), frameon = False )\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "edcc3073",
      "metadata": {
        "id": "edcc3073"
      },
      "source": [
        "## Persistent cohomology analysis with CEBRA embeddings of varying output dimensions:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92fef495",
      "metadata": {
        "id": "92fef495"
      },
      "source": [
        "### Train additional CEBRA-Behavior with dimensions 8, 16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "717fe188",
      "metadata": {
        "id": "717fe188"
      },
      "outputs": [],
      "source": [
        "cebra_posdir8_model= CEBRA(model_architecture='offset10-model',\n",
        "                        batch_size=512,\n",
        "                        learning_rate=3e-4,\n",
        "                        temperature=1,\n",
        "                        output_dimension=8,\n",
        "                        max_iterations=max_iterations,\n",
        "                        distance='cosine',\n",
        "                        conditional='time_delta',\n",
        "                        device='cuda_if_available',\n",
        "                        verbose=True,\n",
        "                        time_offsets=10)\n",
        "cebra_posdir16_model = CEBRA(model_architecture='offset10-model',\n",
        "                        batch_size=512,\n",
        "                        learning_rate=3e-4,\n",
        "                        temperature=1,\n",
        "                        output_dimension=16,\n",
        "                        max_iterations=max_iterations,\n",
        "                        distance='cosine',\n",
        "                        conditional='time_delta',\n",
        "                        device='cuda_if_available',\n",
        "                        verbose=True,\n",
        "                        time_offsets=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "621e40c3",
      "metadata": {
        "id": "621e40c3"
      },
      "outputs": [],
      "source": [
        "cebra_posdir8_model.fit(hippocampus_pos.neural, hippocampus_pos.continuous_index.numpy())\n",
        "cebra_posdir8 = cebra_posdir8_model.transform(hippocampus_pos.neural)\n",
        "\n",
        "cebra_posdir16_model.fit(hippocampus_pos.neural, hippocampus_pos.continuous_index.numpy())\n",
        "cebra_posdir16 = cebra_posdir16_model.transform(hippocampus_pos.neural)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "126fa8e9",
      "metadata": {
        "id": "126fa8e9"
      },
      "source": [
        "### Plotting the results:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba85dcfb",
      "metadata": {
        "id": "ba85dcfb",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize = (10,3), dpi = 100)\n",
        "\n",
        "ax1 = plt.subplot(131,projection='3d')\n",
        "ax2 = plt.subplot(132, projection = '3d')\n",
        "ax3 = plt.subplot(133, projection = '3d')\n",
        "\n",
        "ax1=plot_hippocampus(ax1, cebra_posdir3, hippocampus_pos.continuous_index)\n",
        "ax2=plot_hippocampus(ax2, cebra_posdir8, hippocampus_pos.continuous_index)\n",
        "ax3=plot_hippocampus(ax3, cebra_posdir16, hippocampus_pos.continuous_index)\n",
        "\n",
        "ax1.set_title('CEBRA-Dim 3')\n",
        "ax2.set_title('CEBRA-Dim 8')\n",
        "ax3.set_title('CEBRA-Dim 16')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5d50a20",
      "metadata": {
        "id": "d5d50a20"
      },
      "source": [
        "## Topological Data Analysis:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c2fb06d1",
      "metadata": {
        "id": "c2fb06d1"
      },
      "source": [
        "### Get random 1000 points of embeddings and do persistent cohomology analysis upto H1\n",
        "\n",
        "- note this requires the ripser package.\n",
        "- If you previously installed DREiMac (such as running this notebook before, there are some dependency clashes with ripser, so you need to uninstall it first, and reinstall risper:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8cb76076",
      "metadata": {
        "id": "8cb76076",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "#!pip uninstall -y dreimac\n",
        "#!pip uninstall -y cebra\n",
        "#!pip uninstall -y ripser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51e7440a",
      "metadata": {
        "id": "51e7440a",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "#!pip install ripser\n",
        "\n",
        "#Depending on your system, this can cause errors.\n",
        "#Thus if issues please follow these instructions directly: https://pypi.org/project/ripser/\n",
        "import ripser"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5983f60b",
      "metadata": {
        "id": "5983f60b"
      },
      "source": [
        "- if you get the error `ValueError: numpy.ndarray size changed, may indicate binary incompatibility. Expected 96 from C header, got 80 from PyObject`, please delete the folders from DREiMac and run the cell above again."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47b0a084",
      "metadata": {
        "id": "47b0a084"
      },
      "outputs": [],
      "source": [
        "maxdim=1 ## Set to 2 to compute upto H2. The computing time is considerably longer. For quick demo upto H1, set it to 1.\n",
        "np.random.seed(111)\n",
        "random_idx=np.random.permutation(np.arange(len(cebra_posdir3)))[:1000]\n",
        "topology_dimension = {}\n",
        "for embedding in [cebra_posdir3, cebra_posdir8, cebra_posdir16]:\n",
        "    ripser_output=ripser.ripser(embedding[random_idx], maxdim=maxdim, coeff=47)\n",
        "    dimension = embedding.shape[1]\n",
        "    topology_dimension[dimension] = ripser_output"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "074549b5",
      "metadata": {
        "id": "074549b5"
      },
      "source": [
        "### If you want to visualize the pre-computed persistent co-homology result:\n",
        "\n",
        "- preloading saves quite a bit of time; to adapt to your own datasets please set topology_dimension and random_inx."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a10200d",
      "metadata": {
        "id": "3a10200d"
      },
      "outputs": [],
      "source": [
        "CURRENT_DIR = '/content/'\n",
        "import os\n",
        "\n",
        "preload=pd.read_hdf(os.path.join(CURRENT_DIR,'rat_demo_example_output.h5'))\n",
        "topology_dimension = preload['topology']\n",
        "topology_random_dimension = preload['topology_random']\n",
        "random_idx = preload['embedding']['random_idx']\n",
        "maxdim=2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "729fa2d2",
      "metadata": {
        "id": "729fa2d2"
      },
      "outputs": [],
      "source": [
        "def plot_barcode(topology_result, maxdim):\n",
        "    fig, axs = plt.subplots(maxdim+1, 1, sharex=True, figsize=(7, 8))\n",
        "    axs[0].set_xlim(0,2)\n",
        "    cocycle = [\"Points\", \"Loops\", \"Voids\"]\n",
        "    for k in range(maxdim+1):\n",
        "        bars = topology_result['dgms'][k]\n",
        "        bars[bars == np.inf] = 10\n",
        "        lc = (\n",
        "            np.vstack(\n",
        "                [\n",
        "                    bars[:, 0],\n",
        "                    np.arange(len(bars), dtype=int) * 6,\n",
        "                    bars[:, 1],\n",
        "                    np.arange(len(bars), dtype=int) * 6,\n",
        "                ]\n",
        "            )\n",
        "            .swapaxes(1, 0)\n",
        "            .reshape(-1, 2, 2)\n",
        "        )\n",
        "        line_segments = LineCollection(lc, linewidth=5, color=\"gray\", alpha=0.5)\n",
        "        axs[k].set_ylabel(cocycle[k], fontsize=20)\n",
        "        if k == 0:\n",
        "            axs[k].set_ylim(len(bars) * 6 - 120, len(bars) * 6)\n",
        "        elif k == 1:\n",
        "            axs[k].set_ylim(0, len(bars) * 1 - 30)\n",
        "        elif k == 2:\n",
        "            axs[k].set_ylim(0, len(bars) * 6 + 10)\n",
        "        axs[k].add_collection(line_segments)\n",
        "        axs[k].set_yticks([])\n",
        "        if k == 2:\n",
        "            axs[k].set_xticks(np.linspace(0, 2, 3), np\n",
        "                              .linspace(0, 2, 3), fontsize=15)\n",
        "            axs[k].set_xlabel(\"Lifespan\", fontsize=20)\n",
        "\n",
        "    return fig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31e002b7",
      "metadata": {
        "id": "31e002b7"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "for k in [3,8,16]:\n",
        "    fig=plot_barcode(topology_dimension[k], maxdim)\n",
        "    fig.suptitle(f'Dimension {k}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f203bfba",
      "metadata": {
        "id": "f203bfba"
      },
      "outputs": [],
      "source": [
        "from persim import plot_diagrams\n",
        "\n",
        "def read_lifespan(ripser_output, dim):\n",
        "    dim_diff = ripser_output['dgms'][dim][:, 1] - ripser_output['dgms'][dim][:, 0]\n",
        "    if dim == 0:\n",
        "        return dim_diff[~np.isinf(dim_diff)]\n",
        "    else:\n",
        "        return dim_diff\n",
        "\n",
        "def get_max_lifespan(ripser_output_list, maxdim):\n",
        "    lifespan_dic = {i: [] for i in range(maxdim+1)}\n",
        "    for f in ripser_output_list:\n",
        "        for dim in range(maxdim+1):\n",
        "            lifespan = read_lifespan(f, dim)\n",
        "            lifespan_dic[dim].extend(lifespan)\n",
        "    return [max(lifespan_dic[i]) for i in range(maxdim+1)], lifespan_dic\n",
        "\n",
        "def get_betti_number(ripser_output, shuffled_max_lifespan):\n",
        "    bettis=[]\n",
        "    for dim in range(len(ripser_output['dgms'])):\n",
        "        lifespans=ripser_output['dgms'][dim][:, 1] - ripser_output['dgms'][dim][:, 0]\n",
        "        betti_d = sum(lifespans > shuffled_max_lifespan[dim] * 1.1)\n",
        "        bettis.append(betti_d)\n",
        "    return bettis\n",
        "def plot_lifespan(topology_dgms, shuffled_max_lifespan, ax, label_vis, maxdim):\n",
        "    plot_diagrams(\n",
        "        topology_dgms,\n",
        "        ax=ax,\n",
        "        legend=True,\n",
        "    )\n",
        "\n",
        "    ax.plot(\n",
        "        [\n",
        "            -0.5,\n",
        "            2,\n",
        "        ],\n",
        "        [-0.5 + shuffled_max_lifespan[0], 2 + shuffled_max_lifespan[0]],\n",
        "        color=\"C0\",\n",
        "        linewidth=3,\n",
        "        alpha=0.5,\n",
        "\n",
        "    )\n",
        "    ax.plot(\n",
        "        [\n",
        "            -0.5,\n",
        "            2,\n",
        "        ],\n",
        "        [-0.5 + shuffled_max_lifespan[1], 2 + shuffled_max_lifespan[1]],\n",
        "        color=\"orange\",\n",
        "        linewidth=3,\n",
        "        alpha=0.5,\n",
        "\n",
        "    )\n",
        "    if maxdim == 2:\n",
        "        ax.plot(\n",
        "            [-0.50, 2],\n",
        "            [-0.5 + shuffled_max_lifespan[2], 2 + shuffled_max_lifespan[2]],\n",
        "            color=\"green\",\n",
        "            linewidth=3,\n",
        "            alpha=0.5,\n",
        "        )\n",
        "    ax.set_xlabel(\"Birth\", fontsize=15)\n",
        "    ax.set_xticks([0, 1, 2])\n",
        "    ax.set_xticklabels([0, 1, 2])\n",
        "    ax.tick_params(labelsize=13)\n",
        "    if label_vis:\n",
        "        ax.set_ylabel(\"Death\", fontsize=15)\n",
        "    else:\n",
        "        ax.set_ylabel(\"\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4fc0a38e",
      "metadata": {
        "id": "4fc0a38e"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(18,5))\n",
        "for n, dim in enumerate([3,8,16]):\n",
        "    shuffled_max_lifespan, lifespan_dic = get_max_lifespan(topology_random_dimension[dim], maxdim)\n",
        "    ax = fig.add_subplot(1,3,n+1)\n",
        "    ax.set_title(f'Dimension {dim}')\n",
        "    plot_lifespan(topology_dimension[dim]['dgms'], shuffled_max_lifespan, ax, True, maxdim)\n",
        "    print(f\"Betti No. for dimension {dim}: {get_betti_number(topology_dimension[dim], shuffled_max_lifespan)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91f401a9",
      "metadata": {
        "id": "91f401a9"
      },
      "source": [
        "### Visualize topology-preserving circular coordinates from the first co-cycle.\n",
        "\n",
        "- This requires the DREiMac codebase. Here, we show you how to install their package:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dca4bbf3",
      "metadata": {
        "id": "dca4bbf3"
      },
      "outputs": [],
      "source": [
        "from dreimac import CircularCoords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6d378d5",
      "metadata": {
        "id": "d6d378d5",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "prime = 47\n",
        "dimension = [3,8,16]\n",
        "fig, axs = plt.subplots(1, 3, figsize=(10,3), dpi=200, subplot_kw={'projection': 'polar'})\n",
        "label = hippocampus_pos.continuous_index[random_idx]\n",
        "r_ind = label[:,1]==1\n",
        "l_ind = label[:,2]==1\n",
        "for i, embedding in enumerate([cebra_posdir3, cebra_posdir8, cebra_posdir16]):\n",
        "    rat_emb=embedding[random_idx]\n",
        "    cc = CircularCoords(rat_emb, 1000, prime = prime, )\n",
        "    radial_angle=cc.get_coordinates(cocycle_idx=[0])\n",
        "    r = np.ones(1000)\n",
        "    right=axs[i].scatter(radial_angle[r_ind], r[r_ind], s=5, c = label[r_ind,0], cmap = 'cool')\n",
        "    left=axs[i].scatter(radial_angle[l_ind], r[l_ind], s=5, c = label[l_ind,0], cmap = 'viridis')\n",
        "    axs[i].set_title(f'Dimension {dimension[i]}')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66635f80",
      "metadata": {
        "id": "66635f80"
      },
      "source": [
        "### Visualize Polar coordinate (radial angle) vs. Position"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a91d32cb",
      "metadata": {
        "id": "a91d32cb"
      },
      "source": [
        "###### if you want to preload our data, run the following, otherwise skip this cell:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05f8ffb6",
      "metadata": {
        "id": "05f8ffb6"
      },
      "outputs": [],
      "source": [
        "cebra_pos3 = preload['embedding'][3]\n",
        "cebra_pos8 = preload['embedding'][8]\n",
        "cebra_pos16 = preload['embedding'][16]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a56ef55",
      "metadata": {
        "id": "9a56ef55"
      },
      "outputs": [],
      "source": [
        "radial_angles = {}\n",
        "for i, embedding in enumerate([cebra_pos3, cebra_pos8, cebra_pos16]):\n",
        "    rat_emb=embedding[random_idx]\n",
        "    cc = CircularCoords(rat_emb, 1000, prime = prime, )\n",
        "    out=cc.get_coordinates()\n",
        "    radial_angles[dimension[i]] = out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7dd85f1b",
      "metadata": {
        "id": "7dd85f1b",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "for k in radial_angles.keys():\n",
        "    fig=plt.figure(figsize=(7,6))\n",
        "    plt.subplots_adjust(wspace=0.3)\n",
        "    fig.suptitle(f'Dimension {k}')\n",
        "    ax1=plt.subplot(121)\n",
        "    ax1.scatter(radial_angles[k][r_ind], label[r_ind,0], s=1, c = 'gray')\n",
        "    ax1.set_xlabel('Radial angle [rad]')\n",
        "    ax1.set_ylabel('Position [m]')\n",
        "    ax2=plt.subplot(122)\n",
        "    ax2.scatter(radial_angles[k][l_ind], label[l_ind,0], s=1, c= 'gray')\n",
        "    ax2.set_xlabel('Radial angle [rad]')\n",
        "    ax2.set_ylabel('Position [m]')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "336bfae0",
      "metadata": {
        "id": "336bfae0"
      },
      "source": [
        "### Compute linear correlation between computed coordinate and the position"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66b2346f",
      "metadata": {
        "id": "66b2346f"
      },
      "outputs": [],
      "source": [
        "import sklearn.linear_model\n",
        "def lin_regression(radial_angles, labels):\n",
        "    def _to_cartesian(radial_angles):\n",
        "        x = np.cos(radial_angles)\n",
        "        y = np.sin(radial_angles)\n",
        "        return np.vstack([x,y]).T\n",
        "    cartesian = _to_cartesian(radial_angles)\n",
        "    lin = sklearn.linear_model.LinearRegression()\n",
        "    lin.fit(cartesian, labels)\n",
        "\n",
        "    return lin.score(cartesian, labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e58ca3b4",
      "metadata": {
        "id": "e58ca3b4"
      },
      "outputs": [],
      "source": [
        "for k in radial_angles.keys():\n",
        "    print(f'Dimension {k} Cycle angle into position')\n",
        "    print(f'Right R2: {lin_regression(radial_angles[k][r_ind], label[r_ind,0])}')\n",
        "    print(f'Left R2: {lin_regression(radial_angles[k][l_ind], label[l_ind,0])}')\n",
        "    print(f'Total R2: {lin_regression(radial_angles[k], label[:,0])}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd262c73",
      "metadata": {
        "id": "dd262c73"
      },
      "source": [
        "### Interactive plot to explore co-cycles and circular coordinates.\n",
        "- Top left panel is a lifespan diagram of persistent co-homology analysis.\n",
        "- Bottom left panel is a visualization of the first 2 dimensions of the used 1000 embedding points.\n",
        "- Bottom right panel is a circular coordinate obtained from the co-cycles.\n",
        "- For more information: https://github.com/ctralie/DREiMac"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "209c057a",
      "metadata": {
        "id": "209c057a"
      },
      "outputs": [],
      "source": [
        "## change the embedding to use. Here, we look at 3D embedding for posdir3 (position+dir, dim 3):\n",
        "X=cebra_posdir3[random_idx]\n",
        "\n",
        "%matplotlib notebook\n",
        "rl_stack=np.vstack([X[r_ind], X[l_ind]])\n",
        "label_stack = np.vstack([label[r_ind], label[l_ind]])\n",
        "\n",
        "c1 = plt.get_cmap('cool')\n",
        "C1 = c1(label[r_ind,0])\n",
        "c2 = plt.get_cmap('viridis')\n",
        "C2 = c2(label[l_ind,0])\n",
        "\n",
        "C = np.vstack([C1,C2])\n",
        "\n",
        "def plot_circles(ax):\n",
        "    ax.scatter(rl_stack[:, 0], rl_stack[:, 1], c=C)\n",
        "\n",
        "\n",
        "cc = CircularCoords(rl_stack, 1000, prime = prime)\n",
        "cc.plot_torii(C, coords_info=2, plots_in_one=3, lowerleft_plot=plot_circles)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "dc327929684d2c13e929b2699e1b37518dbb61b921da51c352c926069002ee0e"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}